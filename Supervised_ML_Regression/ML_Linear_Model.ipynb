{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    ElasticNet,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    AdaBoostClassifier,\n",
    "    AdaBoostRegressor,\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Performance of Linear Machine Learning Models: including prediction of new data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "def check_model_performance_linear(ModelClasses, data, dependent_var, drop_columns=[], test_size=0.2, scaler=MinMaxScaler(), new_data=None, random_state=None):\n",
    "    \"\"\"\n",
    "    Check the performance of multiple models on a dataset, and predict values for new data.\n",
    "\n",
    "    Args:\n",
    "        ModelClasses (list): A list of model classes to evaluate.\n",
    "        data (pandas.DataFrame): The dataset to use for training and evaluation.\n",
    "        dependent_var (str): The name of the dependent variable column.\n",
    "        drop_columns (list, optional): A list of column names to drop from the independent variables. Defaults to [].\n",
    "        test_size (float, optional): The proportion of the data to use for testing. Defaults to 0.2.\n",
    "        scaler (object, optional): The scaler to use for scaling the independent variables. Defaults to MinMaxScaler().\n",
    "        new_data (pandas.DataFrame, optional): New data for which to predict values. Defaults to None.\n",
    "        random_state (int, optional): The random seed to use for splitting the data into training and testing sets. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A dataframe with the performance metrics for each model, and predicted values for new data.\n",
    "    \"\"\"\n",
    "    # Define the independent and dependent variables\n",
    "    X = data.drop([dependent_var] + drop_columns, axis=1)\n",
    "    y = data[dependent_var]\n",
    "\n",
    "    # Scale the independent variables\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    \n",
    "    # for-loop for all Linear Supervised Learning Models\n",
    "    results = []\n",
    "    for ModelClass in ModelClasses:\n",
    "        model_name = ModelClass.__name__\n",
    "        \n",
    "        # Triang Models \n",
    "        model = ModelClass().fit(X_scaled, y)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate R^2 \n",
    "        r2_train = r2_score(y_train, y_pred_train)\n",
    "        r2_test = r2_score(y_test, y_pred_test)\n",
    "        \n",
    "        # Calculte RMSE\n",
    "        rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "        rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "        \n",
    "        # Calculate AIC\n",
    "        n = len(y)\n",
    "        k = len(X.columns) + 1  # Add 1 for the intercept term\n",
    "        aic = n * np.log(np.mean((y - model.predict(X_scaled)) ** 2)) + 2 * k\n",
    "        \n",
    "        # Calculate CV RMSE\n",
    "        cv_results = cross_val_score(model, X_scaled, y, cv=5, scoring=make_scorer(mean_squared_error, squared=False))\n",
    "        cv_rmse_mean = cv_results.mean()\n",
    "        cv_rmse_std = cv_results.std()\n",
    "\n",
    "        # Predict values for new data\n",
    "        if new_data is not None:\n",
    "            X_new = new_data.drop(drop_columns, axis=1)\n",
    "            X_new_scaled = scaler.transform(X_new)\n",
    "            y_pred_new = model.predict(X_new_scaled)\n",
    "            new_data_with_predictions = pd.concat([new_data, pd.Series(y_pred_new, name='predicted_'+dependent_var)], axis=1)\n",
    "        else:\n",
    "            new_data_with_predictions = None\n",
    "\n",
    "        \n",
    "        # Wrapping up Performance Metrics\n",
    "        results.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Train R^2\": r2_train,\n",
    "                \"Test R^2\": r2_test,\n",
    "                \"Train RMSE\": rmse_train,\n",
    "                \"Test RMSE\": rmse_test,\n",
    "                \"AIC\": aic,\n",
    "                \"CV RMSE Mean\": cv_rmse_mean,\n",
    "                \"CV RMSE Std\": cv_rmse_std,\n",
    "                \"Predictions for New Data\": new_data_with_predictions\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version of grid_search_best_regression_models_updated\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load\n",
    "import os\n",
    "# Regression models\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    AdaBoostRegressor,\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def ML_Regression_models_with_GridSearch(ModelClasses, hyperparameters, data, dependent_var, drop_columns=[], \n",
    "                                       new_data=None, test_size=0.2, random_state=42, scaler=StandardScaler(), \n",
    "                                       scoring = 'neg_mean_squared_error', cv=3, pca=None, save_model=False, save_dir=None):\n",
    "    \"\"\"\n",
    "    This function performs grid search for the best classification models.\n",
    "\n",
    "    Parameters:\n",
    "    Modelclasses (list): A list of classifier classes.\n",
    "    \n",
    "    Example: use the following code to create a list of model classes\n",
    "    ModelClasses = [\n",
    "        LinearRegression,\n",
    "        Ridge,\n",
    "        Lasso,\n",
    "        DecisionTreeRegressor,\n",
    "        RandomForestRegressor,\n",
    "        GradientBoostingRegressor,\n",
    "        AdaBoostRegressor,\n",
    "        SVR,\n",
    "        KNeighborsRegressor,\n",
    "        XGBRegressor\n",
    "    ]\n",
    "    \n",
    "    hyperparameters (dict): Hyperparameters to be used in grid search.\n",
    "    \n",
    "    Example: use the following code to create a dictionary of hyperparameters\n",
    "    hyperparameters = {\n",
    "     'LinearRegression': {},\n",
    "    'Ridge': {\n",
    "        'alpha': [0.1, 1, 10]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.1, 1, 10]\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'max_depth': [2, 4, 8],\n",
    "        'min_samples_leaf': [1, 2, 5]\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [2, 4, 8]\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [2, 4]\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'n_estimators': [25, 50, 100],\n",
    "        'learning_rate': [0.5, 1, 2]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'XGBRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [2, 4, 6],\n",
    "        'subsample': [0.5, 0.75, 1],\n",
    "    }\n",
    "}\n",
    "\n",
    "    \n",
    "    data (pd.DataFrame): A DataFrame containing the data.\n",
    "    dependent_var (str): Column name of the dependent variable.\n",
    "    drop_columns (list, optional): Column names to be excluded from the analysis.\n",
    "    new_data (pd.DataFrame, optional): DataFrame containing the new data.\n",
    "    test_size (float, optional): Test set size for splitting (default is 0.2).\n",
    "    random_state (int, optional): Random seed for reproducibility (default is 42).\n",
    "    scaler (scikit-learn scaler, optional): Scaler object for feature scaling (default is MinMaxScaler(feature_range=(0, 1))).\n",
    "    scoring (str, optional): Scoring method for cross-validation (default is 'accuracy').\n",
    "    cv (int, optional): Number of folds for cross-validation (default is 5).    \n",
    "    pca (int, optional): Number of components for PCA (default is None).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing model performance metrics.    \n",
    "    \"\"\"\n",
    "    \n",
    "    if drop_columns is None:\n",
    "        drop_columns = []\n",
    "\n",
    "    if not isinstance(ModelClasses, list):\n",
    "        raise TypeError(\"ModelClasses must be a list of classifier classes.\")\n",
    "    if not isinstance(hyperparameters, dict):\n",
    "        raise TypeError(\"hyperparameters must be a dictionary.\")\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise TypeError(\"data must be a pandas DataFrame.\")\n",
    "    if not isinstance(drop_columns, list):\n",
    "        raise TypeError(\"drop_columns must be a list of column names.\")\n",
    "     \n",
    "    # 1. Load and preprocess data\n",
    "    X = data.drop([dependent_var] + drop_columns, axis=1)\n",
    "    y = data[dependent_var]\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Demension Reduction using PCA (Prinscipal Component Analysis)\n",
    "    if pca is not None:\n",
    "        pca_transformer = PCA(n_components=pca)\n",
    "        X_train = pca_transformer.fit_transform(X_train)\n",
    "        X_test = pca_transformer.transform(X_test)\n",
    "\n",
    "    # Scaling\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 2. Run regression models\n",
    "    results = []\n",
    "    \n",
    "    for ModelClass in ModelClasses:\n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(ModelClass(), hyperparameters[ModelClass.__name__], scoring=scoring, cv=cv)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Predict using best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_train_pred = best_model.predict(X_train_scaled)\n",
    "        y_test_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "        # Calculate R2, Train RMSE, Test RMSE, CV RMSE, and CV RMSE Std\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        cv_rmse = np.sqrt(-grid_search.best_score_)\n",
    "        cv_rmse_std = np.sqrt(grid_search.cv_results_['std_test_score'][grid_search.best_index_])\n",
    "\n",
    "        # Predict values for new data\n",
    "        if new_data is not None:\n",
    "            new_X = new_data.drop([dependent_var] + drop_columns, axis=1)\n",
    "            new_y = new_data[dependent_var]\n",
    "\n",
    "            if pca is not None:\n",
    "                new_X = pca_transformer.transform(new_X)\n",
    "                \n",
    "            new_X_scaled = scaler.transform(new_X)\n",
    "            new_y_pred = np.round(best_model.predict(new_X_scaled), 3)\n",
    "            new_y_residual = np.round(new_y - new_y_pred, 3)\n",
    "\n",
    "        else:\n",
    "            new_y_pred = None\n",
    "            new_y_residual = None\n",
    "\n",
    "        # Save the best model for the current ModelClass if save_model is True\n",
    "        if save_model:\n",
    "            model_file_name = f\"{ModelClass.__name__}_best_regression_ML_model.joblib\"\n",
    "            if save_dir is not None:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                model_file_path = os.path.join(save_dir, model_file_name)\n",
    "            else:\n",
    "                model_file_path = model_file_name\n",
    "\n",
    "            dump(best_model, model_file_path)\n",
    "            print(f\"Best model saved to {model_file_path}\")\n",
    "            \n",
    "        # Store results\n",
    "        result = {\n",
    "            \"Title\": \"ML Regression Models with Grid Search\",\n",
    "            \"Model\": ModelClass.__name__,\n",
    "            \"Train R²\": round(train_r2,3),\n",
    "            \"Test R²\": round(test_r2,3),\n",
    "            \"Train RMSE\": round(train_rmse,3),\n",
    "            \"Test RMSE\": round(test_rmse, 3),\n",
    "            \"CV RMSE\": round(cv_rmse, 3),\n",
    "            \"CV RMSE Std\": round(cv_rmse_std,3),\n",
    "            \"Best Hyperparameters\": grid_search.best_params_,\n",
    "            \"New Prediction\": new_y_pred,\n",
    "            \"New Residual\": new_y_residual\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P-values: mathematical computation of feature importace for linear models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "# P-values: mathematical computation of feature importace for linear models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def check_p_values(data, dependent_var, drop_columns=[]):\n",
    "    \"\"\"\n",
    "    Check the p-values of the coefficients for each independent variable in a single model.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The dataset to use for training and evaluation.\n",
    "        dependent_var (str): The name of the dependent variable column.\n",
    "        drop_columns (list, optional): A list of column names to drop from the independent variables. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A dataframe with the p-values for each independent variable.\n",
    "    \"\"\"\n",
    "    # Define the independent and dependent variables\n",
    "    X = data.drop([dependent_var] + drop_columns, axis=1)\n",
    "    y = data[dependent_var]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_ols = sm.add_constant(X_scaled)\n",
    "    model_ols = sm.OLS(y, X_ols).fit()\n",
    "    p_values = model_ols.pvalues[1:]\n",
    "\n",
    "    results_df = pd.DataFrame({'Variable': list(X.columns), 'P-Value': [f'{p:.2f}' for p in p_values], 'Coefficient': model_ols.params[1:].round(2)})\n",
    "\n",
    "    return results_df.set_index('Variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots for checking assumptions (linearity, normality, homoscedasticity) for Supervised Learning models_improved\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def plot_for_checking_assumptions(ModelClasses, data, dependent_var, drop_columns=[], scaler=MinMaxScaler(feature_range=(0,1)), pca=None, savefig=None):\n",
    "    \"\"\"\n",
    "    Create diagnostic plots to check the assumptions of multiple regression models.\n",
    "\n",
    "    Args:\n",
    "        ModelClasses (list): A list of model classes to evaluate.\n",
    "        data (pandas.DataFrame): The dataset to use for training and evaluation.\n",
    "        dependent_var (str): The name of the dependent variable column.\n",
    "        drop_columns (list, optional): A list of column names to drop from the independent variables. Defaults to [].\n",
    "        scaler (object, optional): The scaler to use for scaling the independent variables. Defaults to MinMaxScaler().\n",
    "        pca (PCA, optional): A PCA object to use for dimensionality reduction. Defaults to None.\n",
    "        savefig (str, optional): The filename to save the plot to. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    X = data.drop([dependent_var] + drop_columns, axis=1)\n",
    "    y = data[dependent_var]\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "\n",
    "    # Apply PCA if specified\n",
    "    if pca is not None:\n",
    "        pca.fit(X_scaled)\n",
    "        X_scaled = pca.transform(X_scaled)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=len(ModelClasses), ncols=3, figsize=(16, 4 * len(ModelClasses)))\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "    axes = axes.ravel()  # convert to a 1D array of Axes objects\n",
    "    \n",
    "    for i, ModelClass in enumerate(ModelClasses):\n",
    "        model = ModelClass().fit(X_scaled, y)\n",
    "        y_pred = model.predict(X_scaled)\n",
    "        residuals = y - y_pred\n",
    "        \n",
    "        # Linearity plot\n",
    "        sns.regplot(x=y_pred, y=residuals, lowess=True, ax=axes[i*3])\n",
    "        axes[i*3].set_xlabel('Predicted Values', y=-0.2)\n",
    "        axes[i*3].set_ylabel('Residuals')\n",
    "        axes[i*3].set_title(f'{ModelClass.__name__} Linearity Plot')\n",
    "        axes[i*3].title.set(y=1.05)\n",
    "        # Normality plot\n",
    "        qq = ProbPlot(residuals)\n",
    "        qq.qqplot(line='s', ax=axes[i*3+1])\n",
    "        axes[i*3+1].set_xlabel('Theoretical Quantiles', y=-0.2)\n",
    "        axes[i*3+1].set_ylabel('Standardized Residuals')\n",
    "        axes[i*3+1].set_title(f'{ModelClass.__name__} Normality Plot')\n",
    "        axes[i*3+1].title.set(y=1.05)\n",
    "        # Homoscedasticity plot\n",
    "        bp_test = het_breuschpagan(residuals, X)\n",
    "        sns.regplot(x=y_pred, y=residuals ** 2, lowess=True, ax=axes[i*3+2])\n",
    "        axes[i*3+2].set_xlabel('Predicted Values', y=-0.2)\n",
    "        axes[i*3+2].set_ylabel('Squared Residuals')\n",
    "        axes[i*3+2].set_title(f'{ModelClass.__name__} Homoscedasticity Plot')\n",
    "        axes[i*3+2].text(0.1, 0.9, f'p-value: {bp_test[1]}', transform=axes[i*3+2].transAxes)\n",
    "        axes[i*3+2].title.set(y=1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if savefig:\n",
    "        plt.savefig(savefig)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF for Checking Multicollinarity of Each Independent Variable_Improved\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "def calculate_vif(data, dependent_var, drop_columns=[], vif_threshold=5, scaler=StandardScaler()):\n",
    "    \"\"\"Calculate the variance inflation factor (VIF) for each independent variable in a dataset.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The dataset to use for calculation.\n",
    "        dependent_var (str): The name of the dependent variable column.\n",
    "        drop_columns (list, optional): A list of column names to drop from the independent variables. Defaults to [].\n",
    "        vif_threshold (float, optional): The threshold for detecting high multicollinearity. Defaults to 5.\n",
    "        scaler (object, optional): The scaler to use for scaling the independent variables. Defaults to StandardScaler().\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The VIF values for each independent variable.\n",
    "    \"\"\"\n",
    "    # Data preparation by dropping columns\n",
    "    X = data.drop([dependent_var] + drop_columns, axis=1)\n",
    "    \n",
    "    # Scaling of Data\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    vif = pd.DataFrame()\n",
    "    vif['Variable'] = X.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(X_scaled.values, i) for i in range(X.shape[1])]\n",
    "        \n",
    "    multicollinear_columns = vif[vif['VIF'] > vif_threshold]['Variable'].tolist()\n",
    "    \n",
    "    if multicollinear_columns:\n",
    "        print(f\"The following variables have high multicollinearity with threshold={vif_threshold}:\\n\", multicollinear_columns)\n",
    "    else:\n",
    "        print(\"No variables have high multicollinearity.\")\n",
    "        \n",
    "    return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def impute_nulls(df):\n",
    "    \"\"\"\n",
    "    Impute null values in a Pandas DataFrame based on the data type of each column.\n",
    "    - For float columns, impute with the mean.\n",
    "    - For integer columns, impute with the median.\n",
    "    - For object columns, impute with the mode.\n",
    "    - For datetime columns, impute with the most recent or most frequent date.\n",
    "    - For timedelta columns, impute with the mode.\n",
    "    - For bool columns, impute with the mode.\n",
    "    - For category columns, impute with the mode.\n",
    "    - For complex columns, impute with the mean.\n",
    "    \"\"\"\n",
    "    # Get data types of all columns\n",
    "    dtypes = df.dtypes\n",
    "\n",
    "    # Iterate over all columns\n",
    "    for col in df.columns:\n",
    "        # Check if column contains null values\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            # Get data type of column\n",
    "            dtype = dtypes[col]\n",
    "            # Impute null values based on data type\n",
    "            if dtype == 'float64' or dtype == 'float32' or dtype == 'float16':\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "            elif dtype == 'int64' or dtype == 'int32' or dtype == 'int16' or dtype == 'int8':\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "            elif dtype == 'object':\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            elif dtype == 'datetime64':\n",
    "                df[col].fillna(method='bfill', inplace=True)\n",
    "            elif dtype == 'timedelta64':\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            elif dtype == 'bool':\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            elif dtype.name == 'category':\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            elif dtype == 'complex64' or dtype == 'complex128':\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def drop_outliers_by_zscores(data, column, lower_zscore, upper_zscore, inplace=False):\n",
    "    \"\"\"\n",
    "    Drops rows from a Pandas DataFrame based on z-scores of a given column.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): The input data.\n",
    "    column (str): The name of the column to use for computing z-scores.\n",
    "    lower_zscore (float): The lower z-score boundary.\n",
    "    upper_zscore (float): The upper z-score boundary.\n",
    "    inplace (bool): If True, updates the DataFrame directly. If False, returns a new DataFrame with outliers dropped.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame or None: The modified DataFrame with outliers dropped, if inplace is False;\n",
    "                              None, if inplace is True.\n",
    "    \"\"\"\n",
    "    # Check input arguments\n",
    "    if column not in data.columns:\n",
    "        raise ValueError(\"Column '%s' not found in data.\" % column)\n",
    "    if not np.isfinite(lower_zscore):\n",
    "        raise ValueError(\"Lower z-score boundary must be finite.\")\n",
    "    if not np.isfinite(upper_zscore):\n",
    "        raise ValueError(\"Upper z-score boundary must be finite.\")\n",
    "\n",
    "    # Compute z-scores\n",
    "    z_scores = pd.Series(stats.zscore(data[column]), index=data.index)\n",
    "\n",
    "    # Drop outliers outside boundaries\n",
    "    mask = (z_scores >= upper_zscore) | (z_scores <= lower_zscore)\n",
    "    \n",
    "    if inplace:\n",
    "        data.drop(data[mask].index, inplace=True)\n",
    "        return None\n",
    "    else:\n",
    "        return data.loc[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13439 entries, 0 to 13579\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Suburb         13439 non-null  object \n",
      " 1   Address        13439 non-null  object \n",
      " 2   Rooms          13439 non-null  int64  \n",
      " 3   Type           13439 non-null  object \n",
      " 4   Price          13439 non-null  float64\n",
      " 5   Method         13439 non-null  object \n",
      " 6   SellerG        13439 non-null  object \n",
      " 7   Date           13439 non-null  object \n",
      " 8   Distance       13439 non-null  float64\n",
      " 9   Postcode       13439 non-null  float64\n",
      " 10  Bedroom2       13439 non-null  float64\n",
      " 11  Bathroom       13439 non-null  float64\n",
      " 12  Car            13439 non-null  float64\n",
      " 13  Landsize       13439 non-null  float64\n",
      " 14  BuildingArea   13439 non-null  float64\n",
      " 15  YearBuilt      13439 non-null  float64\n",
      " 16  CouncilArea    13439 non-null  object \n",
      " 17  Lattitude      13439 non-null  float64\n",
      " 18  Longitude      13439 non-null  float64\n",
      " 19  Regionname     13439 non-null  object \n",
      " 20  Propertycount  13439 non-null  float64\n",
      "dtypes: float64(12), int64(1), object(8)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/young78703/Data_Science_Project/data/melb_data.csv')\n",
    "df.rename(columns={'Longtitude':'Longitude'},inplace=True)\n",
    "impute_nulls(df)\n",
    "drop_outliers_by_zscores(df, 'Price', -3.5, 3.5, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train R^2</th>\n",
       "      <th>Test R^2</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>AIC</th>\n",
       "      <th>CV RMSE Mean</th>\n",
       "      <th>CV RMSE Std</th>\n",
       "      <th>Predictions for New Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.446266</td>\n",
       "      <td>0.474300</td>\n",
       "      <td>408543.263528</td>\n",
       "      <td>404829.680746</td>\n",
       "      <td>347242.517509</td>\n",
       "      <td>451496.072889</td>\n",
       "      <td>84279.241050</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.445618</td>\n",
       "      <td>0.474405</td>\n",
       "      <td>408782.502643</td>\n",
       "      <td>404789.174004</td>\n",
       "      <td>347254.622795</td>\n",
       "      <td>410951.767040</td>\n",
       "      <td>10056.270271</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.446269</td>\n",
       "      <td>0.474291</td>\n",
       "      <td>408542.397262</td>\n",
       "      <td>404833.322226</td>\n",
       "      <td>347242.519407</td>\n",
       "      <td>450822.444177</td>\n",
       "      <td>82935.316483</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>0.997915</td>\n",
       "      <td>0.998385</td>\n",
       "      <td>25069.961658</td>\n",
       "      <td>22438.434776</td>\n",
       "      <td>271731.087909</td>\n",
       "      <td>436645.140051</td>\n",
       "      <td>20133.158487</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.961619</td>\n",
       "      <td>0.964847</td>\n",
       "      <td>107558.598016</td>\n",
       "      <td>104684.581158</td>\n",
       "      <td>311278.409799</td>\n",
       "      <td>316051.504448</td>\n",
       "      <td>20594.814390</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>0.684394</td>\n",
       "      <td>0.692974</td>\n",
       "      <td>308432.487331</td>\n",
       "      <td>309379.095003</td>\n",
       "      <td>339752.498029</td>\n",
       "      <td>333510.740139</td>\n",
       "      <td>14078.456391</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoostRegressor</td>\n",
       "      <td>0.366827</td>\n",
       "      <td>0.397019</td>\n",
       "      <td>436866.667504</td>\n",
       "      <td>433566.107433</td>\n",
       "      <td>349052.365158</td>\n",
       "      <td>448495.430560</td>\n",
       "      <td>16822.883903</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVR</td>\n",
       "      <td>-0.066026</td>\n",
       "      <td>-0.072321</td>\n",
       "      <td>566854.230451</td>\n",
       "      <td>578183.571728</td>\n",
       "      <td>356201.997310</td>\n",
       "      <td>569206.230449</td>\n",
       "      <td>37404.790891</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNeighborsRegressor</td>\n",
       "      <td>0.705790</td>\n",
       "      <td>0.714399</td>\n",
       "      <td>297794.351134</td>\n",
       "      <td>298389.082019</td>\n",
       "      <td>338803.313232</td>\n",
       "      <td>401477.709266</td>\n",
       "      <td>20455.178050</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>0.905419</td>\n",
       "      <td>0.906305</td>\n",
       "      <td>168845.286620</td>\n",
       "      <td>170907.328834</td>\n",
       "      <td>323607.341052</td>\n",
       "      <td>278048.432977</td>\n",
       "      <td>18984.426407</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  Train R^2  Test R^2     Train RMSE  \\\n",
       "0           LinearRegression   0.446266  0.474300  408543.263528   \n",
       "1                      Ridge   0.445618  0.474405  408782.502643   \n",
       "2                      Lasso   0.446269  0.474291  408542.397262   \n",
       "3      DecisionTreeRegressor   0.997915  0.998385   25069.961658   \n",
       "4      RandomForestRegressor   0.961619  0.964847  107558.598016   \n",
       "5  GradientBoostingRegressor   0.684394  0.692974  308432.487331   \n",
       "6          AdaBoostRegressor   0.366827  0.397019  436866.667504   \n",
       "7                        SVR  -0.066026 -0.072321  566854.230451   \n",
       "8        KNeighborsRegressor   0.705790  0.714399  297794.351134   \n",
       "9               XGBRegressor   0.905419  0.906305  168845.286620   \n",
       "\n",
       "       Test RMSE            AIC   CV RMSE Mean   CV RMSE Std  \\\n",
       "0  404829.680746  347242.517509  451496.072889  84279.241050   \n",
       "1  404789.174004  347254.622795  410951.767040  10056.270271   \n",
       "2  404833.322226  347242.519407  450822.444177  82935.316483   \n",
       "3   22438.434776  271731.087909  436645.140051  20133.158487   \n",
       "4  104684.581158  311278.409799  316051.504448  20594.814390   \n",
       "5  309379.095003  339752.498029  333510.740139  14078.456391   \n",
       "6  433566.107433  349052.365158  448495.430560  16822.883903   \n",
       "7  578183.571728  356201.997310  569206.230449  37404.790891   \n",
       "8  298389.082019  338803.313232  401477.709266  20455.178050   \n",
       "9  170907.328834  323607.341052  278048.432977  18984.426407   \n",
       "\n",
       "  Predictions for New Data  \n",
       "0                     None  \n",
       "1                     None  \n",
       "2                     None  \n",
       "3                     None  \n",
       "4                     None  \n",
       "5                     None  \n",
       "6                     None  \n",
       "7                     None  \n",
       "8                     None  \n",
       "9                     None  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelClasses = [\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    DecisionTreeRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    SVR,\n",
    "    KNeighborsRegressor,\n",
    "    XGBRegressor\n",
    "]\n",
    "check_model_performance_linear(ModelClasses, df, 'Price', drop_columns=['Suburb', 'Address','Type','Method', 'Bedroom2', 'SellerG','Date','Postcode', 'CouncilArea', 'Lattitude',\n",
    "   'Longitude', 'Regionname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression, Ridge, and Lasso have similar performance metrics, which makes sense as they are variations of linear models. None of them explain more than 50% of the variance in the dependent variable.\n",
    "DecisionTreeRegressor seems to perform exceptionally well on the training data but this could be a sign of overfitting as decision trees tend to memorize the training data.\n",
    "RandomForestRegressor and GradientBoostingRegressor are both ensemble methods and show a balanced performance on training and test data.\n",
    "AdaBoostRegressor has lower R^2 values, indicating a lesser fit compared to the other models.\n",
    "SVR (Support Vector Regression) performs poorly, with negative R^2 values indicating worse predictions than a simple average of the dependent variable.\n",
    "KNeighborsRegressor has moderate R^2 values and could be a candidate for parameter tuning to improve performance.\n",
    "XGBRegressor shows strong performance, with high R^2 values and lower RMSE, indicating a good balance between bias and variance.\n",
    "In summary, the ensemble models, especially XGBRegressor, seem to perform best on this dataset. However, without knowing the context of the data and the scale of the dependent variable, these interpretations should be taken cautiously. It's also important to check for overfitting and ensure that the models are interpretable and applicable to the domain from which the data is drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Metrics Explained\n",
    "R^2 (Coefficient of Determination): Indicates how well the model explains the variability of the target variable. Values range from 0 to 1, with higher values indicating better model performance. It's a measure of the model's explanatory power.\n",
    "RMSE (Root Mean Square Error): Measures the average magnitude of the errors between predicted and actual values, with lower values indicating better fit. It's a standard way to measure the error of a model in predicting quantitative data.\n",
    "AIC (Akaike Information Criterion): Assesses the quality of a model relative to other models, considering both the complexity of the model and its fit to the data. Lower AIC values indicate a better model.\n",
    "CV RMSE Mean (Cross-Validation RMSE Mean): The average RMSE across different folds in cross-validation, providing an estimate of the model's prediction error. Lower values suggest better generalization.\n",
    "CV RMSE Std (Cross-Validation RMSE Standard Deviation): Reflects the variability of the model's prediction error across different folds in cross-validation. Lower values indicate more stable performance.\n",
    "Model Performance Analysis\n",
    "Linear Models (Linear Regression, Ridge, Lasso): Show moderate explanatory power with R^2 values around 0.447 to 0.469. Their RMSE values are relatively high, indicating less precise predictions.\n",
    "Decision Tree Regressor: Exhibits excellent performance with R^2 values close to 1 and very low RMSE, indicating highly accurate predictions.\n",
    "Random Forest and XGBRegressor: Both models demonstrate high R^2 values and relatively low RMSE, suggesting strong predictive accuracy and reliability.\n",
    "Gradient Boosting Regressor and KNeighborsRegressor: Present moderate R^2 values and RMSE, indicating fair predictive performance.\n",
    "AdaBoostRegressor and SVR: These models show lower R^2 values and higher RMSE, suggesting weaker predictive capabilities compared to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ModelClasses = [\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    DecisionTreeRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    SVR,\n",
    "    KNeighborsRegressor,\n",
    "    XGBRegressor]\n",
    "    \n",
    "    hyperparameters = {\n",
    "     'LinearRegression': {},\n",
    "    'Ridge': {\n",
    "        'alpha': [0.1, 1, 10]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.1, 1, 10]\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'max_depth': [2, 4, 8],\n",
    "        'min_samples_leaf': [1, 2, 5]\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [2, 4, 8]\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [2, 4]\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'n_estimators': [25, 50, 100],\n",
    "        'learning_rate': [0.5, 1, 2]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'XGBRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [2, 4, 6],\n",
    "        'subsample': [0.5, 0.75, 1],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Title                      Model  Train R²  \\\n",
      "0  ML Regression Models with Grid Search           LinearRegression     0.454   \n",
      "1  ML Regression Models with Grid Search                      Ridge     0.454   \n",
      "2  ML Regression Models with Grid Search                      Lasso     0.454   \n",
      "3  ML Regression Models with Grid Search      DecisionTreeRegressor     0.634   \n",
      "4  ML Regression Models with Grid Search      RandomForestRegressor     0.708   \n",
      "5  ML Regression Models with Grid Search  GradientBoostingRegressor     0.792   \n",
      "6  ML Regression Models with Grid Search          AdaBoostRegressor     0.461   \n",
      "7  ML Regression Models with Grid Search                        SVR     0.082   \n",
      "8  ML Regression Models with Grid Search        KNeighborsRegressor     0.998   \n",
      "9  ML Regression Models with Grid Search               XGBRegressor     0.881   \n",
      "\n",
      "   Test R²  Train RMSE   Test RMSE     CV RMSE  CV RMSE Std  \\\n",
      "0    0.442  408789.228  403907.399  466445.990   261290.278   \n",
      "1    0.442  408789.349  403904.196  463599.599   254033.166   \n",
      "2    0.442  408789.228  403907.591  466348.830   261044.510   \n",
      "3    0.560  334907.581  358878.219  374003.514    85410.155   \n",
      "4    0.625  299144.280  331417.431  334020.276    45427.723   \n",
      "5    0.728  252461.801  282301.179  286723.643    44297.598   \n",
      "6    0.429  406323.699  408929.703  408824.879    32383.529   \n",
      "7    0.082  530104.380  518307.254  542619.418    69680.249   \n",
      "8    0.550   22859.473  362989.560  374873.999    47695.699   \n",
      "9    0.775  190542.682  256684.588  261717.275    55268.574   \n",
      "\n",
      "                                Best Hyperparameters New Prediction  \\\n",
      "0                                                 {}           None   \n",
      "1                                      {'alpha': 10}           None   \n",
      "2                                      {'alpha': 10}           None   \n",
      "3            {'max_depth': 8, 'min_samples_leaf': 5}           None   \n",
      "4               {'max_depth': 8, 'n_estimators': 50}           None   \n",
      "5  {'learning_rate': 0.1, 'max_depth': 4, 'n_esti...           None   \n",
      "6         {'learning_rate': 0.5, 'n_estimators': 25}           None   \n",
      "7                      {'C': 10, 'kernel': 'linear'}           None   \n",
      "8          {'n_neighbors': 7, 'weights': 'distance'}           None   \n",
      "9  {'learning_rate': 0.1, 'max_depth': 6, 'n_esti...           None   \n",
      "\n",
      "  New Residual  \n",
      "0         None  \n",
      "1         None  \n",
      "2         None  \n",
      "3         None  \n",
      "4         None  \n",
      "5         None  \n",
      "6         None  \n",
      "7         None  \n",
      "8         None  \n",
      "9         None  \n"
     ]
    }
   ],
   "source": [
    "returns= ML_Regression_models_with_GridSearch(ModelClasses, hyperparameters, df, 'Price', drop_columns=['Suburb', 'Address','Type','Method', 'Bedroom2', 'SellerG','Date','Postcode', 'CouncilArea', 'Lattitude',\n",
    "   'Longitude', 'Regionname'],new_data=None, test_size=0.2, random_state=42, scaler=StandardScaler(), scoring = 'neg_mean_squared_error', cv=3, pca=None, save_model=False, save_dir=None)\n",
    "print(returns)\n",
    "returns.to_csv('ML_Linear_Model_Results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
