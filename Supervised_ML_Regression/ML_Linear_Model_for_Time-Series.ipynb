{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    ElasticNet,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    AdaBoostClassifier,\n",
    "    AdaBoostRegressor,\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Performance of Linear Machine Learning Models: including prediction of new data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import os\n",
    "import timeit\n",
    "import joblib\n",
    "\n",
    "def predict_future_prices(model, data, scale_params_df):\n",
    "    # Create a list of symbols in the original order\n",
    "    original_order = scale_params_df['Symbol'].tolist()\n",
    "    \n",
    "    # Convert the scale_params DataFrame into a dictionary for easy lookup\n",
    "    scale_params = scale_params_df.set_index('Symbol').to_dict(orient='index')\n",
    "\n",
    "    # Initialize a list to hold predictions with the symbol as the key\n",
    "    predictions = []\n",
    "\n",
    "    if 'Symbol' in data.columns:\n",
    "        # Group by 'Symbol' and predict for each group\n",
    "        grouped = data.groupby('Symbol')\n",
    "        for symbol, group_data in grouped:\n",
    "            X = group_data.drop(columns=['Symbol'])\n",
    "            if 'Target' in X.columns:\n",
    "                X = X.drop(columns=['Target'])\n",
    "\n",
    "            # Make predictions for the current group\n",
    "            y_pred = model.predict(X)\n",
    "\n",
    "            # Scale the predictions if scale parameters are provided\n",
    "            if symbol in scale_params:\n",
    "                params = scale_params[symbol]\n",
    "                y_pred = (y_pred * params['Target_Std'] + params['Target_Mean']).item()\n",
    "                predictions.append((symbol, y_pred))\n",
    "\n",
    "    else:\n",
    "        print(\"The 'Symbol' column is missing from the data.\")\n",
    "        return None\n",
    "\n",
    "    # Order predictions to match the original order of symbols\n",
    "    ordered_predictions = [{ 'Symbol': symbol, 'Prediction': next((pred for sym, pred in predictions if sym == symbol), None) } for symbol in original_order]\n",
    "\n",
    "    return ordered_predictions\n",
    "\n",
    "def split_data(X, y, train_frac=0.7, valid_frac=0.15, random_state=False):\n",
    "    \"\"\" Split the data into train, validation, and test sets. \"\"\"\n",
    "    total_count = X.shape[0]\n",
    "    train_size = int(total_count * train_frac)\n",
    "    valid_size = int(total_count * valid_frac)\n",
    "    # test_size = total_count - train_size - valid_size\n",
    "\n",
    "    if random_state:\n",
    "\n",
    "        indices = np.random.permutation(total_count)\n",
    "        train_indices = indices[:train_size]\n",
    "        valid_indices = indices[train_size:train_size + valid_size]\n",
    "        test_indices = indices[train_size + valid_size:]\n",
    "\n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_valid, y_valid = X[valid_indices], y[valid_indices]\n",
    "        X_test, y_test = X[test_indices], y[test_indices]\n",
    "\n",
    "    else:\n",
    "        X_train, y_train = X[:train_size], y[:train_size]\n",
    "        X_valid, y_valid = X[train_size:train_size + valid_size], y[train_size:train_size + valid_size]\n",
    "        X_test, y_test = X[train_size + valid_size:], y[train_size + valid_size:]\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "def check_model_performance_linear(ModelClasses, data, dependent_var, drop_columns=[], train_frac=0.7, valid_frac=0.15, scaler=MinMaxScaler(), \n",
    "                                   pca=5, random_state=True, output_file_path=None, save_directory=None):\n",
    "    \"\"\"\n",
    "    Check the performance of multiple models on a dataset, and predict values for new data.\n",
    "\n",
    "    Args:\n",
    "        ModelClasses (list): A list of model classes to evaluate.\n",
    "        data (pandas.DataFrame): The dataset to use for training and evaluation.\n",
    "        dependent_var (str): The name of the dependent variable column.\n",
    "        drop_columns (list, optional): A list of column names to drop from the independent variables. Defaults to [].\n",
    "        train_frac (float, optional): The proportion of the data to use for training. Defaults to 0.7.\n",
    "        valid_frac (float, optional): The proportion of the data to use for validation. Defaults to 0.15.\n",
    "        scaler (object, optional): The scaler to use for scaling the independent variables. Defaults to MinMaxScaler().\n",
    "        new_data (pandas.DataFrame, optional): New data for which to predict values. Defaults to None.\n",
    "        random_state (int, optional): The random seed to use for splitting the data into training and testing sets. Defaults to None.\n",
    "        output_file_path (str, optional): Path to the directory where output files are stored. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A dataframe with the performance metrics for each model, and predicted values for new data.\n",
    "    \"\"\"\n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    if data is None:\n",
    "        # # Load tensors\n",
    "        # X_train = torch.load(os.path.join(output_file_path, 'X_train.pt'))\n",
    "        # y_train = torch.load(os.path.join(output_file_path, 'y_train.pt'))\n",
    "        # X_valid = torch.load(os.path.join(output_file_path, 'X_valid.pt'))\n",
    "        # y_valid = torch.load(os.path.join(output_file_path, 'y_valid.pt'))\n",
    "        # X_test = torch.load(os.path.join(output_file_path, 'X_test.pt'))\n",
    "        # y_test = torch.load(os.path.join(output_file_path, 'y_test.pt'))\n",
    "\n",
    "        X_train = pd.read_csv(os.path.join(output_file_path, 'X_train.csv'))\n",
    "        y_train = pd.read_csv(os.path.join(output_file_path, 'y_train.csv'))\n",
    "        X_valid = pd.read_csv(os.path.join(output_file_path, 'X_valid.csv'))\n",
    "        y_valid = pd.read_csv(os.path.join(output_file_path, 'y_valid.csv'))\n",
    "        X_test = pd.read_csv(os.path.join(output_file_path, 'X_test.csv'))\n",
    "        y_test = pd.read_csv(os.path.join(output_file_path, 'y_test.csv'))\n",
    "\n",
    "        # Demension Reduction using PCA (Prinscipal Component Analysis)\n",
    "        if pca is not None:\n",
    "            pca_transformer = PCA(n_components=pca)\n",
    "            X_train = pca_transformer.fit_transform(X_train)\n",
    "            X_valid = pca_transformer.transform(X_valid)\n",
    "            X_test = pca_transformer.transform(X_test)\n",
    "            \n",
    "    else:\n",
    "        # Define the independent and dependent variables\n",
    "        X = data.drop([dependent_var] + drop_columns, axis=1)\n",
    "        y = data[dependent_var]\n",
    "\n",
    "        # Split the data into training, validation, and testing sets\n",
    "        X_train, y_train, X_valid, y_valid, X_test, y_test = split_data(X, y, train_frac=train_frac, valid_frac=valid_frac, random_state=random_state)\n",
    "\n",
    "        # Demension Reduction using PCA (Prinscipal Component Analysis)\n",
    "        if pca is not None:\n",
    "            pca_transformer = PCA(n_components=pca)\n",
    "            X_train = pca_transformer.fit_transform(X_train)\n",
    "            X_valid = pca_transformer.transform(X_valid)\n",
    "            X_test = pca_transformer.transform(X_test)\n",
    "\n",
    "        # Scale the independent variables\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_valid = scaler.transform(X_valid)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize results list\n",
    "    results = []\n",
    "    results_future = []\n",
    "\n",
    "    # Iterate over each model class\n",
    "    for ModelClass in ModelClasses:\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        \n",
    "        model_name = ModelClass.__name__\n",
    "        \n",
    "        # Initialize and fit the model\n",
    "        model = ModelClass().fit(X_train, y_train)\n",
    "\n",
    "        # Save the model\n",
    "        if save_directory:\n",
    "            model_filename = os.path.join(save_directory, f\"{model_name}.joblib\")\n",
    "            joblib.dump(model, model_filename)\n",
    "            print(f\"Saved {model_name} model to {model_filename}\")\n",
    "\n",
    "        # Predictions on training set\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        r2_train = r2_score(y_train, y_pred_train)\n",
    "        mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "        mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "        \n",
    "        # Predictions on validation set\n",
    "        y_pred_valid = model.predict(X_valid)\n",
    "        r2_valid = r2_score(y_valid, y_pred_valid)\n",
    "        mae_valid = mean_absolute_error(y_valid, y_pred_valid)\n",
    "        mse_valid = mean_squared_error(y_valid, y_pred_valid)\n",
    "        \n",
    "        # Predictions on test set\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        r2_test = r2_score(y_test, y_pred_test)\n",
    "        mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "        mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "        # Metrics calculations...        \n",
    "        # Store results\n",
    "        result=({\n",
    "            \"Model\": model_name,\n",
    "            \"Train R^2\": r2_train,\n",
    "            \"Train MAE\": mae_train,\n",
    "            \"Train MSE\": mse_train,\n",
    "            \"Validation R^2\": r2_valid,\n",
    "            \"Validation MAE\": mae_valid,\n",
    "            \"Validation MSE\": mse_valid,\n",
    "            \"Test R^2\": r2_test,\n",
    "            \"Test MAE\": mae_test,\n",
    "            \"Test MSE\": mse_test,\n",
    "        })\n",
    "        results.append(result)\n",
    "\n",
    "        # Predict future prices\n",
    "        # Load future data\n",
    "        future_data = pd.read_csv(os.path.join(output_file_path, 'future_data.csv'))\n",
    "        scale_params = pd.read_csv(os.path.join(output_file_path, 'scale_params.csv'))\n",
    "\n",
    "        # Assume 'Symbol' is a non-numeric column that we want to exclude from transformations\n",
    "        future_data_numeric = future_data.copy().drop(columns=['Symbol'])\n",
    "\n",
    "        # Apply PCA transformations used on the training data\n",
    "        # future_data_scaled = scaler.transform(future_data_numeric)\n",
    "        if pca is not None:\n",
    "            future_data_pca = pca_transformer.transform(future_data_numeric)\n",
    "            future_data_pca = pd.DataFrame(future_data_pca, columns=future_data.columns)\n",
    "\n",
    "            rescaled_predictions = predict_future_prices(model, future_data_pca, scale_params)\n",
    "\n",
    "        else:\n",
    "            rescaled_predictions = predict_future_prices(model, future_data, scale_params)\n",
    "            \n",
    "        #     # Use the trained model to make predictions\n",
    "        #     future_predictions = model.predict(future_data_pca)\n",
    "        # else:\n",
    "        #     future_predictions = model.predict(future_data_numeric)\n",
    "\n",
    "        # # If future_predictions is 2D but should be 1D, flatten or squeeze it\n",
    "        # if len(future_predictions.shape) > 1 and future_predictions.shape[1] == 1:\n",
    "        #     future_predictions = future_predictions.squeeze()\n",
    "\n",
    "        # future_predictions['Symbol'] = future_data['Symbol']\n",
    "\n",
    "        # # Convert predictions to the original scale \n",
    "        # target_mean = scale_params['Target_Mean']\n",
    "        # target_std = scale_params['Target_Std']\n",
    "        # rescaled_predictions = future_predictions * target_std + target_mean\n",
    "\n",
    "        rescaled_predictions_df = pd.DataFrame(rescaled_predictions)\n",
    "        rescaled_predictions_df['Model'] = model_name\n",
    "        # Create a DataFrame to store the predictions alongside the 'Symbol' column\n",
    "        # prediction_df = pd.DataFrame({\n",
    "        #     # 'Symbol': future_data['Symbol'],\n",
    "        #     \"Model\": model_name,\n",
    "        #     'Predicted_Price': rescaled_predictions\n",
    "        # })\n",
    "        results_future.append(rescaled_predictions_df)\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Symbol_{model_name} is: {duration} seconds\")\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    # Combine all prediction dataframes into a single dataframe\n",
    "    results_future_df = pd.concat(results_future, ignore_index=True)\n",
    "\n",
    "    # Remove duplicate entries from the DataFrame\n",
    "    # results_future_df = results_future_df.drop_duplicates()\n",
    "    \n",
    "    # Sort the dataframe by 'Symbol'\n",
    "    original_order = future_data['Symbol'].unique()\n",
    "    # Now apply the pivot to rearrange the predicted prices side by side for each model\n",
    "    pivoted_results_future_df = results_future_df.pivot(index='Symbol', columns='Model', values='Prediction')\n",
    "    # Reindex the pivot table to maintain the original order\n",
    "    pivoted_results_future_df = pivoted_results_future_df.reindex(original_order)\n",
    "\n",
    "    return results, pivoted_results_future_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #     # Predict future prices\n",
    "    #     # Load future data\n",
    "    #     future_data = pd.read_csv(os.path.join(output_file_path, 'future_data.csv'))\n",
    "    #     scale_params = pd.read_csv(os.path.join(output_file_path, 'scale_params.csv'))\n",
    "\n",
    "    #     # Assume 'Symbol' is a non-numeric column that we want to exclude from transformations\n",
    "    #     future_data_numeric = future_data.drop(columns=['Symbol'])\n",
    "\n",
    "    #     # Apply PCA transformations used on the training data\n",
    "    #     # future_data_scaled = scaler.transform(future_data_numeric)\n",
    "    #     if pca is not None:\n",
    "    #         future_data_pca = pca_transformer.transform(future_data_numeric)\n",
    "    #         future_data_pca = pd.DataFrame(future_data_pca, columns=future_data.columns)\n",
    "\n",
    "    #         rescaled_predictions = predict_future_prices(model, future_data_pca, scale_params)\n",
    "\n",
    "    #     else:\n",
    "    #         rescaled_predictions = predict_future_prices(model, future_data, scale_params)\n",
    "            \n",
    "    #     #     # Use the trained model to make predictions\n",
    "    #     #     future_predictions = model.predict(future_data_pca)\n",
    "    #     # else:\n",
    "    #     #     future_predictions = model.predict(future_data_numeric)\n",
    "\n",
    "    #     # # If future_predictions is 2D but should be 1D, flatten or squeeze it\n",
    "    #     # if len(future_predictions.shape) > 1 and future_predictions.shape[1] == 1:\n",
    "    #     #     future_predictions = future_predictions.squeeze()\n",
    "\n",
    "    #     # future_predictions['Symbol'] = future_data['Symbol']\n",
    "\n",
    "    #     # # Convert predictions to the original scale \n",
    "    #     # target_mean = scale_params['Target_Mean']\n",
    "    #     # target_std = scale_params['Target_Std']\n",
    "    #     # rescaled_predictions = future_predictions * target_std + target_mean\n",
    "\n",
    "    #     # Create a DataFrame to store the predictions alongside the 'Symbol' column\n",
    "    #     prediction_df = pd.DataFrame({\n",
    "    #         'Symbol': future_data['Symbol'],\n",
    "    #         \"Model\": model_name,\n",
    "    #         'Predicted_Price': rescaled_predictions\n",
    "    #     })\n",
    "    #     results_future.append(prediction_df)\n",
    "\n",
    "    #     end = timeit.default_timer()\n",
    "    #     # Calculate and print duration\n",
    "    #     duration = end - start\n",
    "    #     print(f\"Execution Time of Symbol_{model_name} is: {duration} seconds\")\n",
    "\n",
    "    # results = pd.DataFrame(results)\n",
    "    # # results_future = pd.DataFrame(results_future)\n",
    "\n",
    "    # # Combine all prediction dataframes into a single dataframe\n",
    "    # results_future_df = pd.concat(results_future, ignore_index=True)\n",
    "    \n",
    "    # # Remove duplicate entries from the DataFrame\n",
    "    # results_future_df = results_future_df.drop_duplicates()\n",
    "    \n",
    "    # # Sort the dataframe by 'Symbol'\n",
    "    # original_order = future_data['Symbol'].unique()\n",
    "    # # Now apply the pivot to rearrange the predicted prices side by side for each model\n",
    "    # pivoted_results_future_df = results_future_df.pivot(index='Symbol', columns='Model', values='Predicted_Price')\n",
    "    # # Reindex the pivot table to maintain the original order\n",
    "    # pivoted_results_future_df = pivoted_results_future_df.reindex(original_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version of grid_search_best_regression_models_updated\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    AdaBoostRegressor,\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def ML_Regression_models_with_GridSearch(ModelClasses, hyperparameters, data, dependent_var, drop_columns=[], new_data=None, test_size=0.2, \n",
    "                                        random_state=True, scaler=StandardScaler(), scoring = 'neg_mean_squared_error', cv=3, pca=None, save_model=False, \n",
    "                                        save_dir=None, output_file_path=None):\n",
    "    \"\"\"\n",
    "    This function performs grid search for the best classification models.\n",
    "\n",
    "    Parameters:\n",
    "    Modelclasses (list): A list of classifier classes.\n",
    "    \n",
    "    Example: use the following code to create a list of model classes\n",
    "    ModelClasses = [\n",
    "        LinearRegression,\n",
    "        Ridge,\n",
    "        Lasso,\n",
    "        DecisionTreeRegressor,\n",
    "        RandomForestRegressor,\n",
    "        GradientBoostingRegressor,\n",
    "        AdaBoostRegressor,\n",
    "        SVR,\n",
    "        KNeighborsRegressor,\n",
    "        XGBRegressor\n",
    "    ]\n",
    "    \n",
    "    hyperparameters (dict): Hyperparameters to be used in grid search.\n",
    "    \n",
    "    Example: use the following code to create a dictionary of hyperparameters\n",
    "    hyperparameters = {\n",
    "     'LinearRegression': {},\n",
    "    'Ridge': {\n",
    "        'alpha': [0.1, 1, 10]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.1, 1, 10]\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'max_depth': [2, 4, 8],\n",
    "        'min_samples_leaf': [1, 2, 5]\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [2, 4, 8]\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [2, 4]\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'n_estimators': [25, 50, 100],\n",
    "        'learning_rate': [0.5, 1, 2]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'XGBRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [2, 4, 6],\n",
    "        'subsample': [0.5, 0.75, 1],\n",
    "    }\n",
    "}\n",
    "\n",
    "    \n",
    "    data (pd.DataFrame): A DataFrame containing the data.\n",
    "    dependent_var (str): Column name of the dependent variable.\n",
    "    drop_columns (list, optional): Column names to be excluded from the analysis.\n",
    "    new_data (pd.DataFrame, optional): DataFrame containing the new data.\n",
    "    test_size (float, optional): Test set size for splitting (default is 0.2).\n",
    "    random_state (int, optional): Random seed for reproducibility (default is 42).\n",
    "    scaler (scikit-learn scaler, optional): Scaler object for feature scaling (default is MinMaxScaler(feature_range=(0, 1))).\n",
    "    scoring (str, optional): Scoring method for cross-validation (default is 'accuracy').\n",
    "    cv (int, optional): Number of folds for cross-validation (default is 5).    \n",
    "    pca (int, optional): Number of components for PCA (default is None).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing model performance metrics.    \n",
    "    \"\"\"\n",
    "    \n",
    "    if drop_columns is None:\n",
    "        drop_columns = []\n",
    "\n",
    "    if not isinstance(ModelClasses, list):\n",
    "        raise TypeError(\"ModelClasses must be a list of classifier classes.\")\n",
    "    if not isinstance(hyperparameters, dict):\n",
    "        raise TypeError(\"hyperparameters must be a dictionary.\")\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise TypeError(\"data must be a pandas DataFrame.\")\n",
    "    if not isinstance(drop_columns, list):\n",
    "        raise TypeError(\"drop_columns must be a list of column names.\")\n",
    "    \n",
    "    if data is None:\n",
    "        # Load tensors\n",
    "        X_train = torch.load(os.path.join(output_file_path, 'X_train.pt'))\n",
    "        y_train = torch.load(os.path.join(output_file_path, 'y_train.pt'))\n",
    "        X_valid = torch.load(os.path.join(output_file_path, 'X_valid.pt'))\n",
    "        y_valid = torch.load(os.path.join(output_file_path, 'y_valid.pt'))\n",
    "        X_test = torch.load(os.path.join(output_file_path, 'X_test.pt'))\n",
    "        y_test = torch.load(os.path.join(output_file_path, 'y_test.pt'))\n",
    "\n",
    "        # Demension Reduction using PCA (Prinscipal Component Analysis)\n",
    "        if pca is not None:\n",
    "            pca_transformer = PCA(n_components=pca)\n",
    "            X_train = pca_transformer.fit_transform(X_train)\n",
    "            X_valid = pca_transformer.transform(X_valid)\n",
    "            X_test = pca_transformer.transform(X_test)\n",
    "    else:\n",
    "        # Define the independent and dependent variables\n",
    "        X = data.drop([dependent_var] + drop_columns, axis=1)\n",
    "        y = data[dependent_var]\n",
    "\n",
    "        # Split the data into training, validation, and testing sets\n",
    "        X_train, y_train, X_valid, y_valid, X_test, y_test = split_data(X, y, train_frac=train_frac, valid_frac=valid_frac, random_state=random_state)\n",
    "\n",
    "        # Demension Reduction using PCA (Prinscipal Component Analysis)\n",
    "        if pca is not None:\n",
    "            pca_transformer = PCA(n_components=pca)\n",
    "            X_train = pca_transformer.fit_transform(X_train)\n",
    "            X_valid = pca_transformer.transform(X_valid)\n",
    "            X_test = pca_transformer.transform(X_test)\n",
    "\n",
    "        # Scale the independent variables\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_valid = scaler.transform(X_valid)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Run regression models\n",
    "    results = []\n",
    "\n",
    "    \n",
    "    for ModelClass in ModelClasses:\n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(ModelClass(), hyperparameters[ModelClass.__name__], scoring=scoring, cv=cv)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Predict using best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_valid_pred = best_model.predict(X_valid)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Calculate R2, Train RMSE, Test RMSE, CV RMSE, and CV RMSE Std\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        valid_r2 = r2_score(y_valid, y_valid_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        valid_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        # cv_rmse = np.sqrt(-grid_search.best_score_)\n",
    "        # cv_rmse_std = np.sqrt(grid_search.cv_results_['std_test_score'][grid_search.best_index_])\n",
    "\n",
    "        # Predict values for new data\n",
    "        if new_data is not None:\n",
    "            if data is None:\n",
    "                last_X = torch.load(os.path.join(output_file_path, 'last_X.pt'))\n",
    "                last_y = torch.load(os.path.join(output_file_path, 'last_y.pt'))\n",
    "                new_X = torch.load(os.path.join(output_file_path, 'new_X.pt'))\n",
    "                new_y = torch.load(os.path.join(output_file_path, 'new_y.pt'))\n",
    "\n",
    "                        # Demension Reduction using PCA (Prinscipal Component Analysis)\n",
    "                if pca is not None:\n",
    "                    pca_transformer = PCA(n_components=pca)\n",
    "                    last_X = pca_transformer.fit_transform(last_X)\n",
    "                    new_X = pca_transformer.fit_transform(new_X)\n",
    "\n",
    "            else:\n",
    "                \n",
    "                last_X = X_test[-1]\n",
    "                new_X = new_data.drop([dependent_var] + drop_columns, axis=1)\n",
    "                new_y = new_data[dependent_var]\n",
    "\n",
    "            if pca is not None:\n",
    "                new_X = pca_transformer.transform(new_X)\n",
    "                \n",
    "            new_X_scaled = scaler.transform(new_X)\n",
    "            new_y_pred = np.round(best_model.predict(new_X_scaled), 3)\n",
    "            new_y_residual = np.round(new_y - new_y_pred, 3)\n",
    "\n",
    "        else:\n",
    "            new_y_pred = None\n",
    "            new_y_residual = None\n",
    "\n",
    "        # Save the best model for the current ModelClass if save_model is True\n",
    "        if save_model:\n",
    "            model_file_name = f\"{ModelClass.__name__}_best_regression_ML_model.joblib\"\n",
    "            if save_dir is not None:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                model_file_path = os.path.join(save_dir, model_file_name)\n",
    "            else:\n",
    "                model_file_path = model_file_name\n",
    "\n",
    "            dump(best_model, model_file_path)\n",
    "            print(f\"Best model saved to {model_file_path}\")\n",
    "            \n",
    "        # Store results\n",
    "        result = {\n",
    "            \"Title\": \"ML Regression Models with Grid Search\",\n",
    "            \"Model\": ModelClass.__name__,\n",
    "            \"Train R²\": round(train_r2,3),\n",
    "            \"Valid R²\": round(valid_r2, 3),\n",
    "            \"Test R²\": round(test_r2,3),\n",
    "            \"Train RMSE\": round(train_rmse,3),\n",
    "            \"Valid RMSE\": round(valid_rmse, 3),\n",
    "            \"Test RMSE\": round(test_rmse, 3),\n",
    "            # \"CV RMSE\": round(cv_rmse, 3),\n",
    "            # \"CV RMSE Std\": round(cv_rmse_std,3),\n",
    "            \"Best Hyperparameters\": grid_search.best_params_,\n",
    "            \"New Prediction\": new_y_pred,\n",
    "            \"New Residual\": new_y_residual\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "    results = pd.DataFrame(results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelClasses = [\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    DecisionTreeRegressor,\n",
    "    # AdaBoostRegressor,\n",
    "    KNeighborsRegressor,\n",
    "    XGBRegressor,\n",
    "    # RandomForestRegressor,\n",
    "    # GradientBoostingRegressor,\n",
    "    # SVR\n",
    "]\n",
    "train_frac=0.7\n",
    "valid_frac=0.20\n",
    "scaler=StandardScaler()\n",
    "pca=None\n",
    "random_state=True\n",
    "time_series=False\n",
    "new_data=None\n",
    "\n",
    "output_file_path=r\"C:\\Users\\young78703\\Documents\\GitHub\\Machine-Learning-Projects\\Data_save\\ML_Regression\\Time_Series_Lag\\stock_SP_100_indicator_daily_05072024\"\n",
    "\n",
    "# check_model_performance_linear(ModelClasses, df, 'Price', drop_columns=['Suburb', 'Address','Type','Method', 'Bedroom2', 'SellerG','Date','Postcode', 'CouncilArea', 'Lattitude',\n",
    "#    'Longitude', 'Regionname'])\n",
    "save_directory = r'C:\\Users\\young78703\\Documents\\GitHub\\Machine-Learning-Projects\\output\\ML_Regression\\Time_Series_Lag\\stock_SP_100_indicator_daily_05072024'\n",
    "results, results_future = check_model_performance_linear(ModelClasses=ModelClasses, data=None, dependent_var=None, drop_columns=[], scaler=scaler, output_file_path=output_file_path, \n",
    "                                         pca=pca, random_state=random_state, save_directory=save_directory)\n",
    "print(results)\n",
    "print(results_future)\n",
    "\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "results.to_csv(os.path.join(save_directory, 'Results.csv'))\n",
    "results_future.to_csv(os.path.join(save_directory, 'Results_Futures.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ModelClasses = [\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    DecisionTreeRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    SVR,\n",
    "    KNeighborsRegressor,\n",
    "    XGBRegressor]\n",
    "    \n",
    "    hyperparameters = {\n",
    "     'LinearRegression': {},\n",
    "    'Ridge': {\n",
    "        'alpha': [0.1, 1, 10]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.1, 1, 10]\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'max_depth': [2, 4, 8],\n",
    "        'min_samples_leaf': [1, 2, 5]\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [2, 4, 8]\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [2, 4]\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'n_estimators': [25, 50, 100],\n",
    "        'learning_rate': [0.5, 1, 2]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'XGBRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [2, 4, 6],\n",
    "        'subsample': [0.5, 0.75, 1],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns= ML_Regression_models_with_GridSearch(ModelClasses, hyperparameters, df, 'Price', drop_columns=['Suburb', 'Address','Type','Method', 'Bedroom2', 'SellerG','Date','Postcode', 'CouncilArea', 'Lattitude',\n",
    "   'Longitude', 'Regionname'],new_data=None, test_size=0.2, random_state=42, scaler=StandardScaler(), scoring = 'neg_mean_squared_error', cv=3, pca=None, save_model=False, save_dir=None)\n",
    "print(returns)\n",
    "returns.to_csv('ML_Linear_Model_Results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
