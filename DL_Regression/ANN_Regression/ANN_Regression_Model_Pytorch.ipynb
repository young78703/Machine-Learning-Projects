{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import MSELoss\n",
    "from torch import nn, device, save\n",
    "from torch.optim import Adam, AdamW\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import timeit\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Optional\n",
    "\n",
    "def prepare_and_save_data(data, dep_var, drop_columns, test_size=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepares and saves data for modeling.\n",
    "\n",
    "    This function prepares the data for modeling by splitting into train, \n",
    "    validation and test sets, scaling features, and converting to tensors.\n",
    "\n",
    "    Parameters:\n",
    "        data (pandas DataFrame): Dataframe containing all data\n",
    "        dep_var (str): Name of dependent/target variable column  \n",
    "        drop_columns (list): Columns to drop from data\n",
    "        test_size (float): Proportion of data to include in test set\n",
    "        random_state (int): Random state for splitting data\n",
    "\n",
    "    Returns:\n",
    "        X_train_tensor, y_train_tensor, X_valid_tensor, y_valid_tensor, \n",
    "        X_test_tensor, y_test_tensor (torch tensors): Prepared and split data\n",
    "   \"\"\"\n",
    "\n",
    "    # Drop specified columns\n",
    "    if drop_columns:\n",
    "        data = data.drop(columns=drop_columns)\n",
    "    \n",
    "    # Select features and target variable\n",
    "    X = data.drop(columns=[dep_var])\n",
    "    y = data[dep_var]\n",
    "\n",
    "    # Split data into training+validation sets and test set\n",
    "    X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Further split training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert numpy arrays to tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float)\n",
    "    X_valid_tensor = torch.tensor(X_valid_scaled, dtype=torch.float)\n",
    "    y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, X_valid_tensor, y_valid_tensor, X_test_tensor, y_test_tensor\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Artificial Neural Network model class.\n",
    "\n",
    "    This class defines an ANN architecture with multiple hidden layers \n",
    "    and activation functions between each layer. Dropout is also applied\n",
    "    between hidden layers for regularization. L1 and L2 regularization \n",
    "    are supported on the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, num_hidden_layers=2, neurons_per_layer=64, activation_function=torch.nn.ReLU(), \n",
    "                 dropout_rate=0.1, l1_regularization=0, l2_regularization=0):\n",
    "        super(ANNModel, self).__init__()\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.l1_regularization = l1_regularization\n",
    "        self.l2_regularization = l2_regularization\n",
    "\n",
    "        # Build the ANN architecture\n",
    "        layers = [nn.Linear(n_features, neurons_per_layer), activation_function]\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers += [\n",
    "                nn.Linear(neurons_per_layer, neurons_per_layer),\n",
    "                activation_function,\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ]\n",
    "        \n",
    "        # Final layer without activation\n",
    "        layers.append(nn.Linear(neurons_per_layer, 1))\n",
    "        \n",
    "        # Wrap the layers into nn.Sequential\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights of layers using Xavier initialization.\"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                # Describe what this line is doing\n",
    "                nn.init.xavier_uniform_(layer.weight) \n",
    "                # Describe what this line is doing  \n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        \"\"\"Calculate regularization loss from model weights.\"\"\"\n",
    "\n",
    "        l1_loss = 0\n",
    "        # Describe what this variable represents\n",
    "        l2_loss = 0  \n",
    "\n",
    "        for param in self.parameters():\n",
    "            # Describe what this line is doing\n",
    "            l1_loss += torch.norm(param, 1)  \n",
    "            # Describe what this line is doing\n",
    "            l2_loss += torch.norm(param, 2) ** 2\n",
    "\n",
    "        return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for the ANN model.\"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # Describe what this line is doing\n",
    "            x = layer(x) if not isinstance(layer, nn.Dropout) else layer(x)\n",
    "\n",
    "            if any(isinstance(layer, activ) for activ in [torch.nn.ReLU, torch.nn.Tanh, torch.nn.ELU, torch.nn.LeakyReLU, torch.nn.Sigmoid]):\n",
    "                # Describe what this line is doing\n",
    "                x = self.activation_function(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_ANN_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam, criterion=torch.nn.MSELoss, batch_size=32, learning_rate=1e-3, \n",
    "                    patience=10, min_delta=0.0001, max_norm=1.0, num_workers=0, pin_memory=False, validation_frequency=1, load_directory=None, save_directory=None, trial=1):\n",
    "\n",
    "    \"\"\"\n",
    "    Trains an Artificial Neural Network (ANN) model using the specified parameters and data, implementing early stopping and model saving.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The neural network model to train.\n",
    "    - X_train, y_train: Training data and labels.\n",
    "    - X_valid, y_valid: Validation data and labels.\n",
    "    - n_epochs: Number of epochs to train the model.\n",
    "    - optimizer: The optimization algorithm (default: torch.optim.Adam).\n",
    "    - criterion: Loss function (default: torch.nn.MSELoss).\n",
    "    - batch_size: Size of batches for training and validation (default: 32).\n",
    "    - learning_rate: Learning rate for the optimizer (default: 1e-3).\n",
    "    - patience: Number of epochs with no improvement on validation loss to wait before stopping training early (default: 10).\n",
    "    - min_delta: Minimum change in validation loss to qualify as an improvement (default: 0.0001).\n",
    "    - max_norm: Maximum norm for gradient clipping (default: 1.0).\n",
    "    - num_workers: Number of subprocesses to use for data loading (default: 0).\n",
    "    - pin_memory: If True, the data loader will copy tensors into CUDA pinned memory before returning them (default: False).\n",
    "    - validation_frequency: Frequency of validation in terms of number of epochs (default: 1).\n",
    "    - load_directory: Directory from which to load the model if resuming training (default: None).\n",
    "    - save_directory: Directory where to save the model and plots (default: None).\n",
    "    - trial: Identifier for the training trial (default: 1).\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing:\n",
    "    - 'train_losses': List of average training losses per epoch.\n",
    "    - 'val_losses': List of average validation losses per validation cycle.\n",
    "    - 'best_val_loss': Best validation loss achieved during training.\n",
    "    - 'early_stopping_counter': Counter indicating the number of consecutive epochs without improvement in validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train, y_train = X_train.clone().detach().to(device).float(), y_train.clone().detach().to(device).float()\n",
    "    X_valid, y_valid = X_valid.clone().detach().to(device).float(), y_valid.clone().detach().to(device).float()\n",
    "\n",
    "    # Create DataLoader for both training and validation sets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    criterion = criterion().to(device)\n",
    "    scaler = GradScaler() # Enables mixed-precision training for faster computation.\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Lists to hold average losses per epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset gradients to zero for a new optimization step.\n",
    "            with autocast(): # Enable mixed-precision training.\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch.view_as(y_pred)) # Calculate loss.\n",
    "                reg_loss = model.regularization_loss() # Calculate regularization loss.\n",
    "                loss += reg_loss # Combine main loss and regularization loss.\n",
    "            scaler.scale(loss).backward() # Scale loss to prevent underflow in mixed precision.\n",
    "            scaler.unscale_(optimizer) # Unscale gradients before clipping.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm) # Clip gradients to prevent explosion.\n",
    "            scaler.step(optimizer) # Perform optimization step.\n",
    "            scaler.update() # Update scaler for next iteration.\n",
    "\n",
    "            total_train_loss += loss.item() # Accumulate the loss.\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader) # Calculate average training loss for the epoch.\n",
    "        train_losses.append(avg_train_loss)  # Store it for later analysis.\n",
    "\n",
    "    # Perform validation at the specified frequency.\n",
    "        if epoch % validation_frequency == 0:\n",
    "            model.eval() # Set the model to evaluation mode.\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():  # Disable gradient computation.\n",
    "                for X_batch, y_batch in valid_loader:\n",
    "                    y_pred = model(X_batch)\n",
    "                    loss = criterion(y_pred, y_batch.view_as(y_pred)) # Calculate validation loss.\n",
    "                    total_val_loss += loss.item() # Accumulate validation loss.\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(valid_loader) # Calculate average validation loss for this cycle.\n",
    "            val_losses.append(avg_val_loss)  # Store it for later analysis.\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.5f}, Val Loss = {avg_val_loss:.5f}\")\n",
    "\n",
    "            # Early stopping logic.\n",
    "            if avg_val_loss < best_val_loss - min_delta:\n",
    "                best_val_loss = avg_val_loss\n",
    "                early_stopping_counter = 0\n",
    "                if save_directory:\n",
    "                    os.makedirs(load_directory, exist_ok=True)\n",
    "                    save_path = os.path.join(load_directory, f\"ANN_model_trial_{trial}.pt\")\n",
    "                    torch.save(model, save_path) # Save the model if there's an improvement.\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} due to no improvement in validation loss.\")\n",
    "                    break # Exit the training loop if no improvement for 'patience' epochs.\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"early_stopping_counter\": early_stopping_counter\n",
    "    }\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss curves over epochs for a given trial and optionally saves the plot to a directory.\n",
    "\n",
    "    Parameters:\n",
    "    - train_losses (list of float): A list containing the average training loss for each epoch.\n",
    "    - val_losses (list of float): A list containing the average validation loss for each validation cycle.\n",
    "    - trial (int): The trial number, used for titling the plot and naming the saved file.\n",
    "    - save_directory (str, optional): The directory where the plot image will be saved. If None, the plot is not saved.\n",
    "\n",
    "    This function creates a line plot with training and validation losses, providing a visual representation of the model's learning process over epochs. The plot is displayed and optionally saved as a PNG file.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size for the plot.\n",
    "    \n",
    "    # Plot training and validation losses with labels and a specific linewidth for visibility.\n",
    "    plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "    plt.plot(val_losses, label='Valid Loss', linewidth=2)\n",
    "    \n",
    "    # Labeling the x-axis as 'Epoch' and the y-axis as 'Loss', with a specific font size for clarity.\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    \n",
    "    # Adding a legend to distinguish between training and validation losses, with a specified font size.\n",
    "    plt.legend(fontsize=14)\n",
    "    \n",
    "    # Titling the plot with reference to the trial number, enhancing readability and information content.\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})', fontsize=16)\n",
    "    \n",
    "    plt.grid(True)  # Adding a grid to the plot for better visualization of data points.\n",
    "    \n",
    "    # Saving the plot to a file if a save directory is specified.\n",
    "    if save_directory:\n",
    "        os.makedirs(save_directory, exist_ok=True)  # Ensures the directory exists before saving.\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial+1}.png\")  # Constructs the file path.\n",
    "        plt.savefig(save_path)  # Saves the figure to the constructed file path.\n",
    "        print(f\"Plot saved to {save_path}\")  # Prints a confirmation message with the save path.\n",
    "    \n",
    "    plt.show()  # Displays the plot in the notebook or Python script output.\n",
    "\n",
    "import os\n",
    "import timeit\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def model_save_ANN(X_train, y_train, X_valid, y_valid, n_trials=1, load_directory=None, save_directory=None, plot_loss=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs multiple trials of training an Artificial Neural Network (ANN) with randomly selected hyperparameters,\n",
    "    saves the trained models, and optionally plots the training and validation loss curves.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train, y_train: Training data and labels.\n",
    "    - X_valid, y_valid: Validation data and labels.\n",
    "    - n_trials (int): Number of training trials to execute with random hyperparameters.\n",
    "    - load_directory (str, optional): Directory to load existing models from (not used in current implementation).\n",
    "    - save_directory (str, optional): Directory to save trained models and results.\n",
    "    - plot_loss (bool): If True, plots the training and validation loss curves after each trial.\n",
    "\n",
    "    Returns:\n",
    "    - all_results_params_df (pandas.DataFrame): DataFrame containing the parameters and results from all trials\n",
    "      if save_directory is specified. Otherwise, returns a list of dictionaries with the same information.\n",
    "    \"\"\"\n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    all_results_params = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        # Randomly generate hyperparameters\n",
    "        num_hidden_layers = random.choice(range(2, 16))\n",
    "        neurons_per_layer = random.choice(range(16, 128))\n",
    "        dropout_rate = random.choice([0.1, 0.2, 0.3])\n",
    "        batch_size = random.choice(range(16, 128))\n",
    "        learning_rate = 0.0001 * random.choice(range(1, 16))\n",
    "        # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        activation_function = random.choice([torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        optimizer = random.choice([torch.optim.Adam, torch.optim.AdamW])\n",
    "        criterion = random.choice([torch.nn.SmoothL1Loss])\n",
    "        n_epochs = random.choice(range(100, 201))\n",
    "        patience = random.choice(range(5, 11))\n",
    "        min_delta = 0.0001 * random.choice(range(1, 2))\n",
    "        l1_regularization = 0.00000001 * random.choice(range(1, 2))\n",
    "        l2_regularization = l1_regularization\n",
    "\n",
    "        # Define the ANN model\n",
    "        model = ANNModel(n_features=X_train.shape[1], num_hidden_layers=num_hidden_layers,\n",
    "                         neurons_per_layer=neurons_per_layer, activation_function=activation_function,\n",
    "                         dropout_rate=dropout_rate, l1_regularization=l1_regularization, l2_regularization = l2_regularization)\n",
    "\n",
    "        # Train the ANN model\n",
    "        training_results = train_ANN_model(model, X_train, y_train, X_valid, y_valid, n_epochs, \n",
    "                                           batch_size=batch_size, learning_rate=learning_rate,\n",
    "                                           patience=patience, min_delta=min_delta, optimizer=optimizer, criterion=criterion, load_directory=load_directory,\n",
    "                                           save_directory=save_directory, trial=trial)\n",
    "\n",
    "        train_losses, val_losses = training_results['train_losses'], training_results['val_losses']\n",
    "        \n",
    "        # Optionally plot the loss curves\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "\n",
    "        # Collect results and parameters\n",
    "        params = {\n",
    "            \"num_hidden_layers\": num_hidden_layers,\n",
    "            \"neurons_per_layer\": neurons_per_layer,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"activation_function\": activation_function.__class__.__name__,\n",
    "            \"optimizer\": optimizer.__name__,\n",
    "            \"criterion\": criterion.__name__,\n",
    "            \"n_epochs\": n_epochs,\n",
    "            \"patience\": patience,\n",
    "            \"min_delta\": min_delta,\n",
    "            \"l1_regularization\": l1_regularization,\n",
    "            \"l2_regularization\": l2_regularization\n",
    "        }\n",
    "\n",
    "        results_params = {**params, \"trial\": trial + 1, \"train_loss\": train_losses[-1], \"val_loss\": val_losses[-1]}\n",
    "        all_results_params.append(results_params)\n",
    "\n",
    "        # Save the results to a CSV file if a save directory is specified\n",
    "        if save_directory:\n",
    "            all_results_params_df = pd.DataFrame(all_results_params)\n",
    "            all_results_params_df.to_csv(os.path.join(save_directory, \"all_results_params.csv\"), index=False)\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        print(f\"Execution Time for Trial {trial + 1}: {end - start} seconds\")\n",
    "\n",
    "    return all_results_params_df if save_directory else all_results_params\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_models_and_save_results(X_test, y_test, load_directory=None, save_directory=None, results_file_name='evaluation_results.csv'):\n",
    "    \"\"\"\n",
    "    Loads trained models from a specified directory, evaluates them on a test dataset,\n",
    "    calculates performance metrics, and saves the results to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - X_test (torch.Tensor): The test set features, expected to be a PyTorch tensor.\n",
    "    - y_test (torch.Tensor): The test set labels, expected to be a PyTorch tensor.\n",
    "    - load_directory (str): The directory from which to load trained model files.\n",
    "    - save_directory (str, optional): The directory where to save the evaluation results CSV.\n",
    "      If None, the results are not saved to file.\n",
    "    - results_file_name (str): The name of the CSV file to save the results to (default is 'evaluation_results.csv').\n",
    "\n",
    "    Returns:\n",
    "    - results_df (pandas.DataFrame): A DataFrame containing the model names and their evaluation metrics (MAE, MSE, R2).\n",
    "    \n",
    "    This function automates the evaluation process of multiple models, facilitating comparison and analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Determine the device to use for computation.\n",
    "    \n",
    "    results = []  # Initialize an empty list to store the results for each model.\n",
    "    file_names = os.listdir(load_directory)  # List all files in the load directory.\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        model_path = os.path.join(load_directory, file_name)  # Construct the full path to the model file.\n",
    "        loaded_model = torch.load(model_path, map_location=device)  # Load the model onto the specified device.\n",
    "        loaded_model.to(device)  # Ensure the model is on the correct device.\n",
    "        loaded_model.eval()  # Set the model to evaluation mode.\n",
    "        \n",
    "        X_test_tensor = X_test.float().to(device)  # Ensure test data is a float tensor and move to the device.\n",
    "        y_test_tensor = y_test.float().to(device)  # Ensure test labels are a float tensor and move to the device.\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for evaluation.\n",
    "            y_pred_tensor = loaded_model(X_test_tensor)  # Make predictions with the model.\n",
    "            # Adjust shape if necessary, e.g., y_pred_tensor = y_pred_tensor.squeeze()\n",
    "        \n",
    "        y_pred = y_pred_tensor.cpu().numpy()  # Move predictions back to CPU and convert to numpy array for evaluation.\n",
    "        y_true = y_test_tensor.cpu().numpy()  # Convert true labels to numpy array for evaluation.\n",
    "\n",
    "        # Calculate evaluation metrics.\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        # Append results for the current model to the results list.\n",
    "        results.append({'Model': file_name, 'MAE': mae, 'MSE': mse, 'R2': r2})\n",
    "    \n",
    "    results_df = pd.DataFrame(results)  # Convert the list of results into a pandas DataFrame for easy analysis and export.\n",
    "    \n",
    "    # Save the results DataFrame to a CSV file if a save directory is specified.\n",
    "    if save_directory:\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)  # Create the directory if it doesn't exist.\n",
    "        results_path = os.path.join(save_directory, results_file_name)  # Construct the full path to the results CSV.\n",
    "        results_df.to_csv(results_path, index=False)  # Save the DataFrame to CSV.\n",
    "        print(f\"Results saved to {results_path}\")  # Print a confirmation message.\n",
    "    \n",
    "    return results_df  \n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def execute_and_evaluate_ANN_models(data, dep_var, drop_columns, n_trials, top_k_models, save_directory, load_directory):\n",
    "    \"\"\"\n",
    "    Executes a series of steps to train, evaluate, and select the top ANN models based on a dataset. \n",
    "    It involves data preparation, model training with random hyperparameters for a specified number of trials,\n",
    "    evaluation of these models on a test set, and selection of the top performing models based on Mean Absolute Error (MAE).\n",
    "\n",
    "    Parameters:\n",
    "    - data (pandas.DataFrame): The dataset containing features and the target variable.\n",
    "    - dep_var (str): The name of the target variable in the dataset.\n",
    "    - drop_columns (list of str): A list of column names to be dropped from the dataset before training.\n",
    "    - n_trials (int): The number of training trials to conduct with randomly selected hyperparameters.\n",
    "    - top_k_models (int): The number of top models to select based on their performance metric.\n",
    "    - save_directory (str): The directory where to save training results, model files, and evaluation results.\n",
    "    - load_directory (str): The directory from which to load trained model files for evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - top_models_df (pandas.DataFrame): A DataFrame containing the evaluation results of the top k models selected based on MAE.\n",
    "\n",
    "    This function orchestrates the workflow from data preprocessing, model training, model evaluation, \n",
    "    to the selection of the top performing models. It automates the process of experimenting with multiple \n",
    "    ANN architectures and hyperparameters to identify models that achieve the best performance on the given dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory, exist_ok=True)  # Ensure the save directory exists.\n",
    "        \n",
    "    # Prepare data and split into training, validation, and test sets, then save the processed data.\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_and_save_data(data, dep_var, drop_columns, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train models with randomly generated hyperparameters and save the training results and plots.\n",
    "    model_save_ANN(X_train, y_train, X_valid, y_valid, n_trials=n_trials, load_directory=load_directory, save_directory=save_directory, plot_loss=True)\n",
    "    \n",
    "    # Evaluate the trained models on the test set, calculate performance metrics, and save the evaluation results.\n",
    "    evaluate_results_df = evaluate_models_and_save_results(X_test, y_test, load_directory=load_directory, save_directory=save_directory, results_file_name='evaluation_results.csv')\n",
    "    \n",
    "    # Select the top k models based on the lowest MAE score and print the results.\n",
    "    if 'MAE' in evaluate_results_df.columns:\n",
    "        top_models_df = evaluate_results_df.nsmallest(top_k_models, 'MAE')\n",
    "    else:\n",
    "        print(\"MAE column not found in evaluate_results_df.\")\n",
    "    \n",
    "    print(\"Top models based on MAE score:\")\n",
    "    print(top_models_df)\n",
    "    \n",
    "    # Save the evaluation results of the top models for further analysis.\n",
    "    top_models_df.to_csv(os.path.join(save_directory, \"top_models_evaluation.csv\"), index=False)\n",
    "    \n",
    "    return top_models_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "def impute_nulls(df):\n",
    "    \"\"\"\n",
    "    Impute null values in a Pandas DataFrame based on the data type of each column.\n",
    "    - For float columns, impute with the mean.\n",
    "    - For integer columns, impute with the median.\n",
    "    - For object columns, impute with the mode.\n",
    "    - For datetime columns, impute with the most recent or most frequent date.\n",
    "    - For timedelta columns, impute with the mode.\n",
    "    - For bool columns, impute with the mode.\n",
    "    - For category columns, impute with the mode.\n",
    "    - For complex columns, impute with the mean.\n",
    "    \"\"\"\n",
    "    # Get data types of all columns\n",
    "    dtypes = df.dtypes\n",
    "\n",
    "    # Iterate over all columns\n",
    "    for col in df.columns:\n",
    "        # Check if column contains null values\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            # Get data type of column\n",
    "            dtype = dtypes[col]\n",
    "            # Impute null values based on data type\n",
    "            if dtype == 'float64' or dtype == 'float32' or dtype == 'float16':\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "            elif dtype == 'int64' or dtype == 'int32' or dtype == 'int16' or dtype == 'int8':\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "            elif dtype == 'object':\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            elif dtype == 'datetime64':\n",
    "                df[col].fillna(method='bfill', inplace=True)\n",
    "            elif dtype == 'timedelta64':\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            elif dtype == 'bool':\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            elif dtype.name == 'category':\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            elif dtype == 'complex64' or dtype == 'complex128':\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "    return df\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def drop_outliers_by_zscores(data, column, lower_zscore, upper_zscore, inplace=False):\n",
    "    \"\"\"\n",
    "    Drops rows from a Pandas DataFrame based on z-scores of a given column.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): The input data.\n",
    "    column (str): The name of the column to use for computing z-scores.\n",
    "    lower_zscore (float): The lower z-score boundary.\n",
    "    upper_zscore (float): The upper z-score boundary.\n",
    "    inplace (bool): If True, updates the DataFrame directly. If False, returns a new DataFrame with outliers dropped.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame or None: The modified DataFrame with outliers dropped, if inplace is False;\n",
    "                              None, if inplace is True.\n",
    "    \"\"\"\n",
    "    # Check input arguments\n",
    "    if column not in data.columns:\n",
    "        raise ValueError(\"Column '%s' not found in data.\" % column)\n",
    "    if not np.isfinite(lower_zscore):\n",
    "        raise ValueError(\"Lower z-score boundary must be finite.\")\n",
    "    if not np.isfinite(upper_zscore):\n",
    "        raise ValueError(\"Upper z-score boundary must be finite.\")\n",
    "\n",
    "    # Compute z-scores\n",
    "    z_scores = pd.Series(stats.zscore(data[column]), index=data.index)\n",
    "\n",
    "    # Drop outliers outside boundaries\n",
    "    mask = (z_scores >= upper_zscore) | (z_scores <= lower_zscore)\n",
    "    \n",
    "    if inplace:\n",
    "        data.drop(data[mask].index, inplace=True)\n",
    "        return None\n",
    "    else:\n",
    "        return data.loc[~mask]\n",
    "    \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Optional\n",
    "\n",
    "def encode_categorical_column(data: pd.DataFrame, column: str, mapping: Optional[dict] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode a categorical column in the input dataframe using LabelEncoder or mapping.\n",
    "\n",
    "    :param data: input dataframe\n",
    "    :param column: column name to be encoded\n",
    "    :param mapping: optional dictionary defining the mapping for ordinal variables; defaults to None\n",
    "    :return: dataframe with the specified column encoded\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "\n",
    "    if mapping is not None:\n",
    "        # Use mapping for ordinal categorical variables\n",
    "        data[column] = data[column].map(mapping)\n",
    "    else:\n",
    "        # Use LabelEncoder for nominal categorical variables\n",
    "        encoder = LabelEncoder()\n",
    "        encoded = encoder.fit_transform(data[column])\n",
    "        data[column] = encoded\n",
    "\n",
    "    return data\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def one_hot_encode(df, columns):\n",
    "    \"\"\"\n",
    "    Preprocesses categorical columns in a DataFrame using OneHotEncoder.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame to preprocess.\n",
    "        columns (list of str): The names of the categorical columns to encode.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The preprocessed DataFrame with the categorical columns\n",
    "            one-hot encoded and dropped.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the specified columns do not exist in the DataFrame.\n",
    "        ValueError: If any of the specified columns do not contain categorical data.\n",
    "    \"\"\"\n",
    "    # Check that all specified columns exist in the DataFrame\n",
    "    missing_columns = set(columns) - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Columns {missing_columns} not found in DataFrame\")\n",
    "\n",
    "    # Check that all specified columns contain categorical data\n",
    "    non_categorical_columns = [col for col in columns if not (is_categorical_dtype(df[col]) or df[col].dtype == object)]\n",
    "    if non_categorical_columns:\n",
    "        raise ValueError(f\"Columns {non_categorical_columns} do not contain categorical data\")\n",
    "\n",
    "    encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "    encoded_array = encoder.fit_transform(df[columns])\n",
    "\n",
    "    # Create a DataFrame with the one-hot encoded arrays and feature names\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(columns))\n",
    "\n",
    "    # Concatenate the original DataFrame and the encoded DataFrame\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "    # Drop the original categorical columns\n",
    "    df.drop(columns, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/young78703/Data_Science_Project/data/melb_data.csv')\n",
    "df.rename(columns={'Longtitude':'Longitude'},inplace=True)\n",
    "impute_nulls(df)\n",
    "drop_outliers_by_zscores(df, 'Price', -3.5, 3.5, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 3\n",
    "top_k_models = 1\n",
    "load_directory = '/home/young78703/Data_Science_Project/output/ANN_Regression/load_directory'\n",
    "save_directory = '/home/young78703/Data_Science_Project/output/ANN_Regression/save_directory'\n",
    "top_models_df = execute_and_evaluate_ANN_models(df, 'Price', drop_columns=['Suburb', 'Address','Type','Method', 'Bedroom2', 'SellerG','Date','Postcode', 'CouncilArea', 'Lattitude',\n",
    "   'Longitude', 'Regionname'], n_trials=n_trials, top_k_models=top_k_models, load_directory=load_directory, save_directory=save_directory)\n",
    "print(top_models_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/young78703/Data_Science_Project/data/german_credit_data.csv')\n",
    "df=encode_categorical_column(df, 'Saving accounts')\n",
    "impute_nulls(df)\n",
    "df=encode_categorical_column(df, 'Checking account')\n",
    "columns = ['Sex', 'Housing', 'Purpose']\n",
    "df = one_hot_encode(df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 10\n",
    "top_k_models = 3\n",
    "load_directory = '/home/young78703/Data_Science_Project/output/ANN_Regression/load_directory'\n",
    "save_directory = '/home/young78703/Data_Science_Project/output/ANN_Regression/save_directory'\n",
    "top_models_df = execute_and_evaluate_ANN_models(df, 'Credit amount', drop_columns=[], n_trials=n_trials, top_k_models=top_k_models, load_directory=load_directory, save_directory=save_directory)\n",
    "print(top_models_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
