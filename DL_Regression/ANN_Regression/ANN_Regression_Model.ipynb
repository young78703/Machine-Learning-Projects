{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from torch import nn, device, save\n",
    "from torch.optim import Adam, AdamW\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import timeit\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def prepare_and_save_data(data, dep_var, drop_columns, output_file_path, test_size=0.2, random_state=42):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_file_path):\n",
    "        os.makedirs(output_file_path, exist_ok=True)\n",
    "\n",
    "    # Drop specified columns\n",
    "    if drop_columns:\n",
    "        data = data.drop(columns=drop_columns)\n",
    "    \n",
    "    # Select features and target variable\n",
    "    X = data.drop(columns=[dep_var])\n",
    "    y = data[dep_var]\n",
    "\n",
    "    # Split data into training+validation sets and test set\n",
    "    X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Further split training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert numpy arrays to tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float)\n",
    "    X_valid_tensor = torch.tensor(X_valid_scaled, dtype=torch.float)\n",
    "    y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, X_valid_tensor, y_valid_tensor, X_test_tensor, y_test_tensor\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, n_features, num_hidden_layers=2, neurons_per_layer=64, activation_function=F.relu, \n",
    "                 dropout_rate=0.1, l1_regularization=0, l2_regularization=0):\n",
    "        super(ANNModel, self).__init__()\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.l1_regularization = l1_regularization\n",
    "        self.l2_regularization = l2_regularization\n",
    "\n",
    "        # Build the ANN architecture\n",
    "        layers = [nn.Linear(n_features, neurons_per_layer), activation_function()]\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers += [\n",
    "                nn.Linear(neurons_per_layer, neurons_per_layer),\n",
    "                activation_function(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ]\n",
    "        \n",
    "        # Final layer without activation\n",
    "        layers.append(nn.Linear(neurons_per_layer, 1))\n",
    "        \n",
    "        # Wrap the layers into nn.Sequential\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        l1_loss = 0\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss += torch.norm(param, 1)\n",
    "            l2_loss += torch.norm(param, 2) ** 2\n",
    "\n",
    "        return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) if not isinstance(layer, nn.Dropout) else layer(x)\n",
    "            if any(isinstance(layer, activ) for activ in [nn.ReLU, nn.Tanh, nn.ELU, nn.LeakyReLU, nn.Sigmoid]):\n",
    "                x = self.activation_function(x)\n",
    "        return x  \n",
    "    \n",
    "def train_ANN_model(model, X_train, y_train, X_valid, y_valid, n_epochs, batch_size=32, learning_rate=1e-3, \n",
    "                    patience=10, min_delta=0.0001, max_norm=1.0, num_workers=0, pin_memory=False, \n",
    "                    validation_frequency=1, save_directory=None, trial=1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train, y_train = torch.tensor(X_train).float().to(device), torch.tensor(y_train).float().to(device)\n",
    "    X_valid, y_valid = torch.tensor(X_valid).float().to(device), torch.tensor(y_valid).float().to(device)\n",
    "    \n",
    "    # Create TensorDataset and DataLoader for both training and validation sets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = MSELoss()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch.view_as(y_pred))\n",
    "                # Add regularization\n",
    "                reg_loss = model.regularization_loss()\n",
    "                loss += reg_loss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # Validation\n",
    "        if epoch % validation_frequency == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in valid_loader:\n",
    "                    y_pred = model(X_batch)\n",
    "                    loss = criterion(y_pred, y_batch.view_as(y_pred))\n",
    "                    val_losses.append(loss.item())\n",
    "\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            print(f\"Epoch {epoch}: Train Loss = {np.mean(train_losses):.5f}, Val Loss = {avg_val_loss:.5f}\")\n",
    "\n",
    "            if avg_val_loss < best_val_loss - min_delta:\n",
    "                best_val_loss = avg_val_loss\n",
    "                early_stopping_counter = 0\n",
    "                # Save model\n",
    "                if save_directory:\n",
    "                    os.makedirs(save_directory, exist_ok=True)\n",
    "                    save_path = os.path.join(save_directory, f\"ANN_model_trial_{trial}_version_{model_ver}.pt\")\n",
    "                else:\n",
    "                    save_path = f\"ANN_model_trial_{trial}.pt\"\n",
    "                save(model.state_dict(), save_path)\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} due to no improvement in validation loss.\")\n",
    "                    break\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"early_stopping_counter\": early_stopping_counter\n",
    "    }\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "    plt.plot(val_losses, label='Valid Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})', fontsize=16)\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_directory:\n",
    "        os.makedirs(save_directory, exist_ok=True)  # Ensure directory exists\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial+1}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def model_save_ANN(X_train, y_train, X_valid, y_valid, n_trials=1, save_directory=None, plot_loss=True):\n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    all_results_params = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        # Randomly generate hyperparameters\n",
    "        num_hidden_layers = random.choice(range(1, 4))\n",
    "        neurons_per_layer = random.choice([64, 128, 256])\n",
    "        dropout_rate = random.choice([0.1, 0.2, 0.3])\n",
    "        batch_size = random.choice([32, 64, 128])\n",
    "        learning_rate = random.choice([1e-3, 1e-4, 1e-5])\n",
    "        activation_function = random.choice([F.relu, F.leaky_relu])\n",
    "        optimizer_choice = random.choice([Adam, AdamW])\n",
    "        n_epochs = random.choice(range(50, 101))\n",
    "        patience = 10\n",
    "\n",
    "        # # Load data\n",
    "        # X_train = torch.load(os.path.join(output_file_path, 'X_train.pt'))\n",
    "        # y_train = torch.load(os.path.join(output_file_path, 'y_train.pt'))\n",
    "        # X_valid = torch.load(os.path.join(output_file_path, 'X_valid.pt'))\n",
    "        # y_valid = torch.load(os.path.join(output_file_path, 'y_valid.pt'))\n",
    "\n",
    "        # Define the ANN model\n",
    "        model = ANNModel(n_features=X_train.shape[1], num_hidden_layers=num_hidden_layers,\n",
    "                         neurons_per_layer=neurons_per_layer, activation_function=activation_function,\n",
    "                         dropout_rate=dropout_rate)\n",
    "\n",
    "        # Train the ANN model\n",
    "        training_results = train_ANN_model(model, X_train, y_train, X_valid, y_valid, n_epochs, \n",
    "                                           batch_size=batch_size, learning_rate=learning_rate,\n",
    "                                           patience=patience, optimizer=optimizer_choice, \n",
    "                                           save_directory=save_directory, trial=trial)\n",
    "\n",
    "        train_losses, val_losses = training_results['train_losses'], training_results['val_losses']\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "\n",
    "        # Collect results and parameters\n",
    "        params = {\n",
    "            \"num_hidden_layers\": num_hidden_layers,\n",
    "            \"neurons_per_layer\": neurons_per_layer,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"optimizer\": optimizer_choice.__name__,\n",
    "            \"n_epochs\": n_epochs\n",
    "        }\n",
    "\n",
    "        results_params = {**params, \"trial\": trial + 1, \"train_loss\": train_losses[-1], \"val_loss\": val_losses[-1]}\n",
    "        all_results_params.append(results_params)\n",
    "\n",
    "        if save_directory:\n",
    "            all_results_params_df = pd.DataFrame(all_results_params)\n",
    "            all_results_params_df.to_csv(os.path.join(save_directory, \"all_results_params.csv\"), index=False)\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        print(f\"Execution Time for Trial {trial + 1}: {end - start} seconds\")\n",
    "\n",
    "    return all_results_params_df if save_directory else all_results_params\n",
    "\n",
    "def evaluate_models_and_save_results(X_test, y_test, load_directory=None, save_directory=None, results_file_name='evaluation_results.csv'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    results = []\n",
    "    file_names = os.listdir(load_directory)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        model_path = os.path.join(load_directory, file_name)\n",
    "        loaded_model = torch.load(model_path, map_location=device)\n",
    "        loaded_model.eval()\n",
    "        \n",
    "        # X_test_tensor = torch.load(os.path.join(output_file_path, 'X_valid.pt'))\n",
    "        # y_test_tensor = torch.load(os.path.join(output_file_path, 'y_valid.pt'))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_tensor = loaded_model(X_test)\n",
    "            # Adjust shape if necessary, e.g., y_pred_tensor = y_pred_tensor.squeeze()\n",
    "        \n",
    "        y_pred = y_pred_tensor.cpu().numpy()\n",
    "        y_true = y_test.cpu().numpy()\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        results.append({'Model': file_name, 'MAE': mae, 'MSE': mse, 'R2': r2})\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if save_directory:\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        results_path = os.path.join(save_directory, results_file_name)\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"Results saved to {results_path}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def execute_and_evaluate_ANN_models(data, dep_var, drop_columns, n_trials, top_k_models, save_directory, load_directory):\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_and_save_data(data, dep_var, drop_columns, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Step 1: Train Models and Save Training Results\n",
    "    model_save_ANN(X_train, y_train, X_valid, y_valid, n_trials=n_trials, save_directory=save_directory, plot_loss=True)\n",
    "    \n",
    "    # Step 2: Load and Evaluate Models, then save evaluation results\n",
    "    evaluate_results_df = evaluate_models_and_save_results(X_test, y_test, load_directory=load_directory, save_directory=save_directory,\n",
    "                                                           results_file_name='evaluation_results.csv')\n",
    "    \n",
    "    # Step 3: Select top k models based on a criterion (e.g., R2 score here for illustration)\n",
    "    top_models_df = evaluate_results_df.nlargest(top_k_models, 'R2')\n",
    "    print(\"Top models based on R2 score:\")\n",
    "    print(top_models_df)\n",
    "    \n",
    "    # Save the top models' evaluation results for further inspection\n",
    "    top_models_df.to_csv(os.path.join(save_directory, \"top_models_evaluation.csv\"), index=False)\n",
    "    \n",
    "    return top_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 10\n",
    "top_k_models = 3\n",
    "load_directory = 'models.txt'\n",
    "save_directory = 'models.txt'\n",
    "top_models_df = execute_and_evaluate_ANN_models(n_trials=n_trials, top_k_models=top_k_models, save_directory=save_directory)\n",
    "print(top_models_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_and_save_results(load_directory=None, output_file_path=None, save_directory=None, results_file_name='evaluation_results.csv'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    results = []\n",
    "    file_names = os.listdir(load_directory)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        model_path = os.path.join(load_directory, file_name)\n",
    "        loaded_model = torch.load(model_path, map_location=device)\n",
    "        loaded_model.eval()\n",
    "        \n",
    "        X_test_tensor = torch.load(os.path.join(output_file_path, 'X_valid.pt'))\n",
    "        y_test_tensor = torch.load(os.path.join(output_file_path, 'y_valid.pt'))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_tensor = loaded_model(X_test_tensor)\n",
    "            # Adjust shape if necessary, e.g., y_pred_tensor = y_pred_tensor.squeeze()\n",
    "        \n",
    "        y_pred = y_pred_tensor.cpu().numpy()\n",
    "        y_true = y_test_tensor.cpu().numpy()\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        results.append({'Model': file_name, 'MAE': mae, 'MSE': mse, 'R2': r2})\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if save_directory:\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        results_path = os.path.join(save_directory, results_file_name)\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"Results saved to {results_path}\")\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_and_evaluate_ANN_models(X_train_path, y_train_path, X_valid_path, y_valid_path, X_test_path, y_test_path,\n",
    "                                    n_trials, top_k_models, save_directory):\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Train Models and Save Training Results\n",
    "    model_save_ANN(n_trials=n_trials, save_directory=save_directory, plot_loss=True,\n",
    "                   output_file_path=output_file_path)\n",
    "    \n",
    "    # Step 2: Load and Evaluate Models, then save evaluation results\n",
    "    evaluate_results_df = evaluate_models_and_save_results(load_directory=load_directory,\n",
    "                                                           output_file_path=output_file_path,\n",
    "                                                           save_directory=save_directory,\n",
    "                                                           results_file_name='evaluation_results.csv')\n",
    "    \n",
    "    # Step 3: Select top k models based on a criterion (e.g., R2 score here for illustration)\n",
    "    top_models_df = evaluate_results_df.nlargest(top_k_models, 'R2')\n",
    "    print(\"Top models based on R2 score:\")\n",
    "    print(top_models_df)\n",
    "    \n",
    "    # Save the top models' evaluation results for further inspection\n",
    "    top_models_df.to_csv(os.path.join(save_directory, \"top_models_evaluation.csv\"), index=False)\n",
    "    \n",
    "    return top_models_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      2\u001b[0m save_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_models\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m all_results_params_df \u001b[38;5;241m=\u001b[39m model_save_ANN(n_trials\u001b[38;5;241m=\u001b[39mn_trials, model_save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_directory\u001b[38;5;241m=\u001b[39msave_directory, plot_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_file_path\u001b[38;5;241m=\u001b[39moutput_file_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save results_df\u001b[39;00m\n\u001b[1;32m      5\u001b[0m all_results_params_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_all_results_params_df.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_file_path' is not defined"
     ]
    }
   ],
   "source": [
    "n_trials = 3\n",
    "save_directory = 'saved_models'\n",
    "all_results_params_df = model_save_ANN(n_trials=n_trials, model_save=True, save_directory=save_directory, plot_loss=True, output_file_path=output_file_path)\n",
    "# Save results_df\n",
    "all_results_params_df.to_csv(f'{save_directory}_all_results_params_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 23:10:25.280227: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-07 23:10:25.282307: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-07 23:10:25.302332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-07 23:10:25.806519: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#  ANN_regression_with_random_search_modify\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def create_model_regression(n_features, dropout_rate=0.2, activation_function='relu', \n",
    "                 optimizer='adam', num_hidden_layers=2, num_neurons_per_layer=64,\n",
    "                 kernel_regularizer=None, early_stop_patience=None, verbose=None):\n",
    "    \"\"\"\n",
    "    Create a deep learning regression model.\n",
    "\n",
    "    Args:\n",
    "    n_features (int): Number of input features.\n",
    "    dropout_rate (float): Dropout rate.\n",
    "    activation_function (str): Activation function.\n",
    "    optimizer (str): Deep learning optimizer.\n",
    "    num_hidden_layers (int): Number of hidden layers.\n",
    "    num_neurons_per_layer (int): Number of neurons in hidden layers.\n",
    "    kernel_regularizer (Optional[Keras Regularizer]): Regularization function for the layers.\n",
    "\n",
    "    Returns:\n",
    "    Keras model: A deep learning regression model with the specified architecture.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(n_features, activation=activation_function, input_dim=n_features, kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(Dense(num_neurons_per_layer, activation=activation_function, kernel_regularizer=kernel_regularizer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "def ANN_regression_with_random_search(data, dep_var, drop_columns=[], n_iter=50, cv=3,\n",
    "                                                    verbose=1, dropout_rates=[0.1, 0.2, 0.3, 0.4],\n",
    "                                                    activation_functions=[\"relu\", \"tanh\"],\n",
    "                                                    optimizers=[\"adam\", \"rmsprop\", \"adagrad\"],\n",
    "                                                    batch_sizes=[16, 32, 64, 128], epochs=[50, 100, 200],\n",
    "                                                    early_stop_patience=[5, 10, 15],\n",
    "                                                    num_hidden_layers_range=range(1, 4),\n",
    "                                                    neurons_per_layer_range=[16, 32, 48, 64],\n",
    "                                                    learning_rate_range=[0.001, 0.01, 0.1],\n",
    "                                                    model_save=True, save_directory=None, plot_loss=True,\n",
    "                                                    test_size=0.2, random_state=42, new_data=None):\n",
    "    \"\"\"\n",
    "    Runs a deep learning regression process with Randomized Search for hyperparameter optimization.\n",
    "\n",
    "    Args:\n",
    "    Required:\n",
    "    data (pd.DataFrame): Data to be used for model training.\n",
    "    dep_var (str): Target variable.\n",
    "\n",
    "    Optional:\n",
    "    drop_columns (List[str]): Columns to be dropped from dataframe.\n",
    "    n_iter (int): Number of iterations for RandomizedSearchCV.\n",
    "    cv (int): Number of cross validation folds.\n",
    "    verbose (int): Print log level.\n",
    "    activation_functions (List[str]): Activation functions to be used.\n",
    "    dropout_rates (List[float]): Dropout rates.\n",
    "    optimizers (List[str]): Deep learning optimizers to be used.\n",
    "    batch_sizes (List[int]): Batch sizes to be used.\n",
    "    epochs (List[int]): Number of epochs to be used.\n",
    "    early_stop_patience (List[int]): Early stopping patience values.\n",
    "    num_hidden_layers_range (List[int]): Number of hidden layer values to search.\n",
    "    neurons_per_layer_range (List[int]): Number of neurons per layer values.\n",
    "    learning_rate_range (List[float]): Learning rates.\n",
    "    model_save (bool): If True, save the model.\n",
    "    save_directory (Optional[str]): Path to the directory to save the model.\n",
    "    test_size (float): Test set proportion.\n",
    "    random_state (int): Random state for train_test_split.\n",
    "    new_data (pd.DataFrame): If provided, predict target values for this data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe with model results.\n",
    "    \"\"\"\n",
    "    # drop columns\n",
    "    X = data.drop([dep_var] + drop_columns, axis=1).values\n",
    "    y = data[dep_var].values\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # get number of features\n",
    "    n_features = X_train_scaled.shape[1]\n",
    "    # create model\n",
    "    deep_learning_model = KerasRegressor(model=create_model_regression, model__n_features=n_features, verbose=verbose)\n",
    "    \n",
    "    # Define the random search parameters\n",
    "    param_dist = {\n",
    "        \"model__activation_function\": activation_functions,\n",
    "        \"model__dropout_rate\": dropout_rates,\n",
    "        \"model__optimizer\": optimizers,\n",
    "        \"model__num_hidden_layers\": num_hidden_layers_range,\n",
    "        \"model__num_neurons_per_layer\": neurons_per_layer_range,\n",
    "        \"model__kernel_regularizer\": [L1L2(l1=0, l2=regularization_strength) for regularization_strength in learning_rate_range],\n",
    "        \"batch_size\": batch_sizes,\n",
    "        \"epochs\": epochs,\n",
    "        \"model__early_stop_patience\": early_stop_patience\n",
    "    }\n",
    "    \n",
    "    # Create the RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(deep_learning_model, param_distributions=param_dist, n_iter=n_iter,\n",
    "                                       cv=cv, error_score=np.nan, verbose=verbose)\n",
    "    \n",
    "    # Create EarlyStopping and ModelCheckpoint callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=early_stop_patience)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=verbose, save_best_only=True)\n",
    "    \n",
    "    # Fit to the training data\n",
    "    random_search.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),callbacks=[es, mc])\n",
    "    \n",
    "    # Get the best model from the RandomizedSearchCV\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    # Get the best parameters from the RandomizedSearchCV\n",
    "    best_params = random_search.best_params_\n",
    "    \n",
    "    # create and fit final model with best parameters\n",
    "    final_model = create_model_regression(n_features=n_features, dropout_rate=best_params['model__dropout_rate'],\n",
    "                                           activation_function=best_params['model__activation_function'],\n",
    "                                           optimizer=best_params['model__optimizer'],\n",
    "                                           num_hidden_layers=best_params['model__num_hidden_layers'],\n",
    "                                           num_neurons_per_layer=best_params['model__num_neurons_per_layer'],\n",
    "                                           kernel_regularizer=best_params['model__kernel_regularizer'],\n",
    "                                           early_stop_patience=best_params['model__early_stop_patience'])\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=best_params['model__early_stop_patience'])\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=verbose, save_best_only=True)\n",
    "    history = final_model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),\n",
    "                              epochs=best_params['epochs'], batch_size=best_params['batch_size'], callbacks=[es, mc])\n",
    "    \n",
    "    # Plotting the training and validation loss\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Loss Plot')\n",
    "        plt.show()\n",
    "\n",
    "    # Save the model\n",
    "    if model_save:\n",
    "        model_save_path = os.path.join(save_directory, 'best_regression_ANN_model_RandomSearch.h5') if save_directory else 'best_regression_ANN_model_RandomSearch.h5'\n",
    "        final_model.save(model_save_path)\n",
    "\n",
    "    # Predict target values for training and test data\n",
    "    y_pred_train = final_model.predict(X_train_scaled)\n",
    "    y_pred_test = final_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Concatenate X_train_scaled and X_test_scaled back into X_scaled\n",
    "    X_scaled = np.concatenate((X_train_scaled, X_test_scaled), axis=0)\n",
    "\n",
    "    # Concatenate y_train and y_test back into a y\n",
    "    y = np.concatenate((y_train, y_test), axis=0)\n",
    "    \n",
    "    # Wrap the Keras model in a scikit-learn estimator\n",
    "    estimator = KerasRegressor(model=final_model, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Calculate CV RMSE and its standard deviation using the cross_val_score for the best model\n",
    "    cv_scores = cross_val_score(estimator, X_scaled, y, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse_scores = np.sqrt(-cv_scores)\n",
    "    cv_rmse = np.mean(rmse_scores)\n",
    "    cv_rmse_std = np.std(rmse_scores)\n",
    "    \n",
    "    # Predict values for new data\n",
    "    if new_data is not None:\n",
    "        new_X = new_data.drop([dep_var] + drop_columns, axis=1).values\n",
    "        new_y = new_data[dep_var].values\n",
    "        new_X_scaled = scaler.fit_transform(new_X)\n",
    "        new_y_pred = final_model.predict(new_X_scaled)\n",
    "        new_y_residual = new_y - new_y_pred.flatten()\n",
    "    else:\n",
    "        new_y_pred = None\n",
    "        new_y_residual = None\n",
    "    \n",
    "    # Create a dataframe with results\n",
    "    results = {\n",
    "        \"Model\": [\"ANN Regression with Random Search (Best Model)\"],\n",
    "        \"Train R²\": [round(train_r2, 3)],\n",
    "        \"Test R²\": [round(test_r2, 3)],\n",
    "        \"Train RMSE\": [round(train_rmse, 3)],\n",
    "        \"Test RMSE\": [round(test_rmse, 3)],\n",
    "        \"CV RMSE\": [round(cv_rmse, 3)],\n",
    "        \"CV RMSE Std\": [round(cv_rmse_std, 3)],\n",
    "        \"Best Hyperparameters\": [best_params],\n",
    "        \"New Predicted\": [new_y_pred] if new_y_pred is not None else [None],\n",
    "        \"New Residual\": [new_y_residual] if new_y_residual is not None else [None]\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      2\u001b[0m save_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_models\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m all_results_params_df \u001b[38;5;241m=\u001b[39m model_save_ANN(n_trials\u001b[38;5;241m=\u001b[39mn_trials, model_save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_directory\u001b[38;5;241m=\u001b[39msave_directory, plot_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_file_path\u001b[38;5;241m=\u001b[39moutput_file_path, model_ver\u001b[38;5;241m=\u001b[39mmodel_ver)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save results_df\u001b[39;00m\n\u001b[1;32m      5\u001b[0m all_results_params_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_all_results_params_df.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_file_path' is not defined"
     ]
    }
   ],
   "source": [
    "n_trials = 3\n",
    "save_directory = 'saved_models'\n",
    "all_results_params_df = model_save_ANN(n_trials=n_trials, model_save=True, save_directory=save_directory, plot_loss=True, output_file_path=output_file_path, model_ver=model_ver)\n",
    "# Save results_df\n",
    "all_results_params_df.to_csv(f'{save_directory}_all_results_params_df.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
