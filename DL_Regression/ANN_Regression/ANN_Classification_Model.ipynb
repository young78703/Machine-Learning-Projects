{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN_classification_with_random_search_modified_updated.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "def create_model_classification(n_features, unique_classes, dropout_rate=0.2, activation_function='relu', \n",
    "                 optimizer='adam', num_hidden_layers=2, num_neurons_per_layer=64,\n",
    "                 kernel_regularizer=None, early_stop_patience=None, verbose=None):\n",
    "    \"\"\"\n",
    "    Create a deep learning model.\n",
    "\n",
    "    Args:\n",
    "    n_features (int): Number of input features.\n",
    "    unique_classes (int): Number of unique target classes.\n",
    "    dropout_rate (float): Dropout rate.\n",
    "    activation_function (str): Activation function.\n",
    "    optimizer (str): Deep learning optimizer.\n",
    "    num_hidden_layers (int): Number of hidden layers.\n",
    "    num_neurons_per_layer (int): Number of neurons in hidden layers.\n",
    "    kernel_regularizer (Optional[Keras Regularizer]): Regularization function for the layers.\n",
    "    early_stop_patience (Optional[int]): Number of epochs with no improvement after which training will be stopped.\n",
    "    verbose (Optional[int]): Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "    \n",
    "    Returns:\n",
    "    Keras model: A deep learning model with the specified architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    if unique_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "        output_activation = 'sigmoid'\n",
    "        n_output_neurons = 1\n",
    "    else:\n",
    "        loss = 'categorical_crossentropy'\n",
    "        output_activation = 'softmax'\n",
    "        n_output_neurons = unique_classes\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(n_features, activation=activation_function, input_dim=n_features, kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(Dense(num_neurons_per_layer, activation=activation_function, kernel_regularizer=kernel_regularizer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(n_output_neurons, activation=output_activation))\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "\n",
    "    return model\n",
    "\n",
    "def ANN_classification_with_random_search(data, dep_var, drop_columns=[], n_iter=50, cv=3, verbose=1,\n",
    "                                                    activation_functions=[\"relu\", \"tanh\"],\n",
    "                                                    dropout_rates=[0.1, 0.2, 0.3, 0.4],\n",
    "                                                    optimizers=[\"adam\", \"rmsprop\", \"adagrad\"],\n",
    "                                                    batch_sizes=[16, 32, 64, 128], epochs=[50, 100, 200],\n",
    "                                                    early_stop_patience=[5, 10, 15],\n",
    "                                                    num_hidden_layers_range = range(1, 4),\n",
    "                                                    neurons_per_layer_range = [16, 32, 48, 64],\n",
    "                                                    learning_rate_range = [0.001, 0.01, 0.1],\n",
    "                                                    model_save=True, save_directory=None, plot_loss=True,\n",
    "                                                    test_size=0.2, random_state=42, new_data=None):\n",
    "    \"\"\"\n",
    "    Runs a deep learning classification process with Randomized Search for hyperparameter optimization.\n",
    "\n",
    "    Args:\n",
    "    Required:\n",
    "    data (pd.DataFrame): Data to be used for model training.\n",
    "    dep_var (str): Target variable.\n",
    "\n",
    "    Optional:\n",
    "    drop_columns (List[str]): Columns to be dropped from dataframe.\n",
    "    n_iter (int): Number of iterations for RandomizedSearchCV.\n",
    "    cv (int): Number of cross validation folds.\n",
    "    verbose (int): Print log level.\n",
    "    activation_functions (List[str]): Activation functions to be used.\n",
    "    dropout_rates (List[float]): Dropout rates.\n",
    "    optimizers (List[str]): Deep learning optimizers to be used.\n",
    "    batch_sizes (List[int]): Batch sizes to be used.\n",
    "    epochs (List[int]): Number of epochs to be used.\n",
    "    early_stop_patience (List[int]): Early stopping patience values.\n",
    "    num_hidden_layers_range (List[int]): Number of hidden layer values to search.\n",
    "    neurons_per_layer_range (List[int]): Number of neurons per layer values.\n",
    "    learning_rate_range (List[float]): Learning rates.\n",
    "    model_save (bool): If True, save the model.\n",
    "    save_directory (Optional[str]): Path to the directory to save the model.\n",
    "    test_size (float): Test set proportion.\n",
    "    random_state (int): Random state for train_test_split.\n",
    "    new_data (pd.DataFrame): If provided, predict target values for this data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe with model results.\n",
    "    \"\"\"\n",
    "    # drop columns\n",
    "    X = data.drop([dep_var] + drop_columns, axis=1).values\n",
    "    y = data[dep_var].values\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # one hot encode target variable if more than 2 classes\n",
    "    unique_classes = len(np.unique(data[dep_var].values))\n",
    "    if unique_classes > 2:\n",
    "        y_train = to_categorical(y_train, num_classes=unique_classes)\n",
    "        y_test = to_categorical(y_test, num_classes=unique_classes)\n",
    "    \n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # get number of features\n",
    "    n_features = X_train_scaled.shape[1]\n",
    "    \n",
    "    # create model\n",
    "    deep_learning_model = KerasClassifier(model=create_model_classification, model__n_features=n_features,\n",
    "                                          model__unique_classes=unique_classes, verbose=verbose)\n",
    "    \n",
    "    # Define the random search parameters\n",
    "    param_dist = {\n",
    "        \"model__activation_function\": activation_functions,\n",
    "        \"model__dropout_rate\": dropout_rates,\n",
    "        \"model__optimizer\": optimizers,\n",
    "        \"model__num_hidden_layers\": num_hidden_layers_range,\n",
    "        \"model__num_neurons_per_layer\": neurons_per_layer_range,\n",
    "        \"model__kernel_regularizer\": [L1L2(l1=0, l2=regularization_strength) for regularization_strength in learning_rate_range],\n",
    "        \"batch_size\": batch_sizes,\n",
    "        \"epochs\": epochs,\n",
    "        \"model__early_stop_patience\": early_stop_patience\n",
    "    }\n",
    "    \n",
    "    # Create the RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(deep_learning_model, param_distributions=param_dist, n_iter=n_iter,\n",
    "                                       cv=cv, error_score=np.nan, verbose=verbose)\n",
    "    \n",
    "    # Create EarlyStopping and ModelCheckpoint callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=early_stop_patience)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=verbose, save_best_only=True)\n",
    "    \n",
    "    # Fit to the training data\n",
    "    random_search.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),callbacks=[es, mc])\n",
    "    \n",
    "    # Get the best model from the RandomizedSearchCV\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    # Get the best parameters from the RandomizedSearchCV\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    # create and fit final model with best parameters\n",
    "    final_model = create_model_classification(n_features=n_features, unique_classes=unique_classes, \n",
    "                                            dropout_rate=best_params['model__dropout_rate'],\n",
    "                                           activation_function=best_params['model__activation_function'],\n",
    "                                           optimizer=best_params['model__optimizer'],\n",
    "                                           num_hidden_layers=best_params['model__num_hidden_layers'],\n",
    "                                           num_neurons_per_layer=best_params['model__num_neurons_per_layer'],\n",
    "                                           kernel_regularizer=best_params['model__kernel_regularizer'],\n",
    "                                           early_stop_patience=best_params['model__early_stop_patience'])\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=best_params['model__early_stop_patience'])\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=verbose, save_best_only=True)\n",
    "    history = final_model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),\n",
    "                              epochs=best_params['epochs'], batch_size=best_params['batch_size'], callbacks=[es, mc])\n",
    "    \n",
    "    # Plotting the training and validation loss\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Loss Plot')\n",
    "        plt.show()\n",
    "\n",
    "    # Save the model\n",
    "    if model_save:\n",
    "        model_save_path = os.path.join(save_directory, 'best_classification_ANN_model_RandomSearch.h5') if save_directory else 'best_classification_ANN_model_RandomSearch.h5'\n",
    "        final_model.save(model_save_path)\n",
    "\n",
    "    # Predict target values for training and test data\n",
    "    if unique_classes > 2:\n",
    "        y_pred_train = np.argmax(final_model.predict(X_train_scaled), axis=1)\n",
    "        y_pred_test = np.argmax(final_model.predict(X_test_scaled), axis=1)\n",
    "        # Use inverse_transform only if you use OneHotEncoder during preprocessing\n",
    "        y_train_original = np.argmax(y_train, axis=1)\n",
    "        y_test_original = np.argmax(y_test, axis=1)\n",
    "    else:\n",
    "        y_pred_train = (final_model.predict(X_train_scaled) > 0.5).astype('int32')\n",
    "        y_pred_test = (final_model.predict(X_test_scaled) > 0.5).astype('int32')\n",
    "        # No need for inverse_transform, use original y_train and y_test\n",
    "        y_train_original = y_train\n",
    "        y_test_original = y_test\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train_original, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test_original, y_pred_test)\n",
    "    conf_mat = confusion_matrix(y_test_original, y_pred_test)\n",
    "    class_report = classification_report(y_test_original, y_pred_test, output_dict=True, zero_division=1)\n",
    "    \n",
    "    # Predict values for new data\n",
    "    if new_data is not None:\n",
    "        new_X = new_data.drop([dep_var] + drop_columns, axis=1)\n",
    "        new_y = new_data[dep_var].values\n",
    "\n",
    "        new_X_scaled = scaler.fit_transform(new_X)\n",
    "        # Predict target values for new data\n",
    "        if unique_classes > 2:\n",
    "            new_y_pred = np.argmax(final_model.predict(new_X_scaled), axis=1)\n",
    "        else:\n",
    "            new_y_pred = (final_model.predict(new_X_scaled) > 0.5).astype('int32')\n",
    "\n",
    "        new_y_residual = new_y - new_y_pred.flatten()\n",
    "    else:\n",
    "        new_y_pred = None\n",
    "        new_y_residual = None\n",
    "    \n",
    "    # Create a simple report\n",
    "    simple_report = {\n",
    "        'Accuracy': round(class_report['accuracy'], 3),\n",
    "        'Precision': round(class_report['macro avg']['precision'], 3),\n",
    "        'Recall': round(class_report['macro avg']['recall'], 3),\n",
    "        'F1-score': round(class_report['macro avg']['f1-score'], 3),\n",
    "        'Supoort': round(class_report['macro avg']['support'], 3)\n",
    "    }\n",
    "    \n",
    "    # Create a dataframe with results\n",
    "    results = {\n",
    "        \"Model\": [\"ANN Classification with Random Search(Best Model)\"],\n",
    "        \"Train Accuracy\": [train_accuracy],\n",
    "        \"Test Accuracy\": [test_accuracy],\n",
    "        \"Confusion Matrix\": [conf_mat.tolist()],\n",
    "        \"Classification Report\": [simple_report],\n",
    "        \"Hyperparameters\": [best_params],\n",
    "        \"New Predicted\": [new_y_pred] if new_y_pred is not None else [None],\n",
    "        \"New Residual\": [new_y_residual] if new_y_residual is not None else [None]\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search best deep learning classification model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "def create_model_classification(n_features, unique_classes, dropout_rate=0.2, activation_function='relu', \n",
    "                 optimizer='adam', num_hidden_layers=2, num_neurons_per_layer=64,\n",
    "                 kernel_regularizer=None, early_stop_patience=None, verbose=None):\n",
    "    \"\"\"\n",
    "    Create a deep learning model.\n",
    "\n",
    "    Args:\n",
    "    n_features (int): Number of input features.\n",
    "    unique_classes (int): Number of unique target classes.\n",
    "    dropout_rate (float): Dropout rate.\n",
    "    activation_function (str): Activation function.\n",
    "    optimizer (str): Deep learning optimizer.\n",
    "    num_hidden_layers (int): Number of hidden layers.\n",
    "    num_neurons_per_layer (int): Number of neurons in hidden layers.\n",
    "    kernel_regularizer (Optional[Keras Regularizer]): Regularization function for the layers.\n",
    "    early_stop_patience (Optional[int]): Number of epochs with no improvement after which training will be stopped.\n",
    "    verbose (Optional[int]): Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "    \n",
    "    Returns:\n",
    "    Keras model: A deep learning model with the specified architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    if unique_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "        output_activation = 'sigmoid'\n",
    "        n_output_neurons = 1\n",
    "    else:\n",
    "        loss = 'categorical_crossentropy'\n",
    "        output_activation = 'softmax'\n",
    "        n_output_neurons = unique_classes\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(n_features, activation=activation_function, input_dim=n_features, kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(Dense(num_neurons_per_layer, activation=activation_function, kernel_regularizer=kernel_regularizer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(n_output_neurons, activation=output_activation))\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
    "\n",
    "    return model\n",
    "\n",
    "def ANN_classification_with_grid_search(data, dep_var, drop_columns=[], n_iter=50, cv=3, verbose=1,\n",
    "                                                    activation_functions=[\"relu\", \"tanh\"],\n",
    "                                                    dropout_rates=[0.1, 0.2, 0.3, 0.4],\n",
    "                                                    optimizers=[\"adam\", \"rmsprop\", \"adagrad\"],\n",
    "                                                    batch_sizes=[16, 32, 64, 128], epochs=[50, 100, 200],\n",
    "                                                    early_stop_patience=[5, 10, 15],\n",
    "                                                    num_hidden_layers_range = range(1, 4),\n",
    "                                                    neurons_per_layer_range = [16, 32, 48, 64],\n",
    "                                                    learning_rate_range = [0.001, 0.01, 0.1],\n",
    "                                                    model_save=True, save_directory=None, plot_loss=True,\n",
    "                                                    test_size=0.2, random_state=42, new_data=None):\n",
    "    \"\"\"\n",
    "    Runs a deep learning classification process with Grid Search for hyperparameter optimization.\n",
    "\n",
    "    Args:\n",
    "    Required:\n",
    "    data (pd.DataFrame): Data to be used for model training.\n",
    "    dep_var (str): Target variable.\n",
    "\n",
    "    Optional:\n",
    "    drop_columns (List[str]): Columns to be dropped from dataframe.\n",
    "    n_iter (int): Number of iterations for RandomizedSearchCV.\n",
    "    cv (int): Number of cross validation folds.\n",
    "    verbose (int): Print log level.\n",
    "    activation_functions (List[str]): Activation functions to be used.\n",
    "    dropout_rates (List[float]): Dropout rates.\n",
    "    optimizers (List[str]): Deep learning optimizers to be used.\n",
    "    batch_sizes (List[int]): Batch sizes to be used.\n",
    "    epochs (List[int]): Number of epochs to be used.\n",
    "    early_stop_patience (List[int]): Early stopping patience values.\n",
    "    num_hidden_layers_range (List[int]): Number of hidden layer values to search.\n",
    "    neurons_per_layer_range (List[int]): Number of neurons per layer values.\n",
    "    learning_rate_range (List[float]): Learning rates.\n",
    "    save_directory (Optional[str]): Path to the directory to save the model.\n",
    "    test_size (float): Test set proportion.\n",
    "    random_state (int): Random state for train_test_split.\n",
    "    new_data (pd.DataFrame): If provided, predict target values for this data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe with model results.\n",
    "    \"\"\"\n",
    "    # drop columns\n",
    "    X = data.drop([dep_var] + drop_columns, axis=1).values\n",
    "    y = data[dep_var].values\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # one hot encode target variable if more than 2 classes\n",
    "    unique_classes = len(np.unique(data[dep_var].values))\n",
    "    if unique_classes > 2:\n",
    "        y_train = to_categorical(y_train, num_classes=unique_classes)\n",
    "        y_test = to_categorical(y_test, num_classes=unique_classes)\n",
    "    \n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # get number of features\n",
    "    n_features = X_train_scaled.shape[1]\n",
    "    \n",
    "    # create model\n",
    "    deep_learning_model = KerasClassifier(model=create_model_classification, model__n_features=n_features,\n",
    "                                          model__unique_classes=unique_classes, verbose=verbose)\n",
    "    \n",
    "    # Define the grid search parameters\n",
    "    param_grid = {\n",
    "        \"model__activation_function\": activation_functions,\n",
    "        \"model__dropout_rate\": dropout_rates,\n",
    "        \"model__optimizer\": optimizers,\n",
    "        \"model__num_hidden_layers\": num_hidden_layers_range,\n",
    "        \"model__num_neurons_per_layer\": neurons_per_layer_range,\n",
    "        \"model__kernel_regularizer\": [L1L2(l1=0, l2=regularization_strength) for regularization_strength in learning_rate_range],\n",
    "        \"batch_size\": batch_sizes,\n",
    "        \"epochs\": epochs,\n",
    "        \"model__early_stop_patience\": early_stop_patience\n",
    "    }\n",
    "    \n",
    "    # Create the RandomizedSearchCV object\n",
    "    random_search = GridSearchCV(deep_learning_model, param_grid=param_grid, \n",
    "                                       cv=cv, error_score=np.nan, verbose=verbose)\n",
    "    \n",
    "    # Create EarlyStopping and ModelCheckpoint callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=early_stop_patience)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=verbose, save_best_only=True)\n",
    "    \n",
    "    # Fit to the training data\n",
    "    random_search.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),callbacks=[es, mc])\n",
    "    \n",
    "    # Get the best model from the RandomizedSearchCV\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    # Get the best parameters from the RandomizedSearchCV\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    # create and fit final model with best parameters\n",
    "    final_model = create_model_classification(n_features=n_features, unique_classes=unique_classes, \n",
    "                                            dropout_rate=best_params['model__dropout_rate'],\n",
    "                                           activation_function=best_params['model__activation_function'],\n",
    "                                           optimizer=best_params['model__optimizer'],\n",
    "                                           num_hidden_layers=best_params['model__num_hidden_layers'],\n",
    "                                           num_neurons_per_layer=best_params['model__num_neurons_per_layer'],\n",
    "                                           kernel_regularizer=best_params['model__kernel_regularizer'],\n",
    "                                           early_stop_patience=best_params['model__early_stop_patience'])\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=best_params['model__early_stop_patience'])\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=verbose, save_best_only=True)\n",
    "    history = final_model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),\n",
    "                              epochs=best_params['epochs'], batch_size=best_params['batch_size'], callbacks=[es, mc])\n",
    "    \n",
    "    # Plotting the training and validation loss\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Loss Plot')\n",
    "        plt.show()\n",
    "\n",
    "    # Save the model\n",
    "    if model_save:\n",
    "        model_save_path = os.path.join(save_directory, 'best_classification_ANN_model_with_GridSearch.h5') if save_directory else 'best_classification_ANN_model_GridSearch.h5'\n",
    "        final_model.save(model_save_path)\n",
    "\n",
    "    # Predict target values for training and test data\n",
    "    if unique_classes > 2:\n",
    "        y_pred_train = np.argmax(final_model.predict(X_train_scaled), axis=1)\n",
    "        y_pred_test = np.argmax(final_model.predict(X_test_scaled), axis=1)\n",
    "        # Use inverse_transform only if you use OneHotEncoder during preprocessing\n",
    "        y_train_original = np.argmax(y_train, axis=1)\n",
    "        y_test_original = np.argmax(y_test, axis=1)\n",
    "    else:\n",
    "        y_pred_train = (final_model.predict(X_train_scaled) > 0.5).astype('int32')\n",
    "        y_pred_test = (final_model.predict(X_test_scaled) > 0.5).astype('int32')\n",
    "        # No need for inverse_transform, use original y_train and y_test\n",
    "        y_train_original = y_train\n",
    "        y_test_original = y_test\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train_original, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test_original, y_pred_test)\n",
    "    conf_mat = confusion_matrix(y_test_original, y_pred_test)\n",
    "    class_report = classification_report(y_test_original, y_pred_test, output_dict=True, zero_division=1)\n",
    "    \n",
    "    # Predict values for new data\n",
    "    if new_data is not None:\n",
    "        new_X = new_data.drop([dep_var] + drop_columns, axis=1).values\n",
    "        new_y = new_data[dep_var].values\n",
    "\n",
    "        new_X_scaled = scaler.fit_transform(new_X)\n",
    "        # Predict target values for new data\n",
    "        if unique_classes > 2:\n",
    "            new_y_pred = np.argmax(final_model.predict(new_X_scaled), axis=1)\n",
    "        else:\n",
    "            new_y_pred = (final_model.predict(new_X_scaled) > 0.5).astype('int32')\n",
    "\n",
    "        new_y_residual = new_y - new_y_pred.flatten()\n",
    "    else:\n",
    "        new_y_pred = None\n",
    "        new_y_residual = None\n",
    "    \n",
    "    # Create a simple report\n",
    "    simple_report = {\n",
    "        'Accuracy': round(class_report['accuracy'], 3),\n",
    "        'Precision': round(class_report['macro avg']['precision'], 3),\n",
    "        'Recall': round(class_report['macro avg']['recall'], 3),\n",
    "        'F1-score': round(class_report['macro avg']['f1-score'], 3),\n",
    "        'Supoort': round(class_report['macro avg']['support'], 3)\n",
    "    }\n",
    "    \n",
    "    # Create a dataframe with results\n",
    "    results = {\n",
    "        \"Model\": [\"ANN Classification with Grid Search(Best Model)\"],\n",
    "        \"Train Accuracy\": [train_accuracy],\n",
    "        \"Test Accuracy\": [test_accuracy],\n",
    "        \"Confusion Matrix\": [conf_mat.tolist()],\n",
    "        \"Classification Report\": [simple_report],\n",
    "        \"Hyperparameters\": [best_params],\n",
    "        \"New Predicted\": [new_y_pred] if new_y_pred is not None else [None],\n",
    "        \"New Residual\": [new_y_residual] if new_y_residual is not None else [None]\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/young78703/Data_Science_Project/data/iris.csv')\n",
    "new_data = df.sample(5)\n",
    "# Preprocess a categorical column (ordinal variable) using mapping\n",
    "# Define a dictionary to map the categorical values to integers\n",
    "mapping = {'setosa': 0, 'versicolor': 1, 'virginica': 2}\n",
    "\n",
    "# Use the map method to apply the dictionary to the column\n",
    "df['species'] = df['species'].map(mapping)\n",
    "\n",
    "result=ANN_classification_with_random_search(df, 'species', new_data=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
