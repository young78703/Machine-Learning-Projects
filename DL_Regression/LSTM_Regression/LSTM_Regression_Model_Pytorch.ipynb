{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        seq = data[i:i+seq_len]\n",
    "        target = data[i+seq_len:i+seq_len+1]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "def prepare_and_save_data(data, dep_var, drop_columns, output_file_path, test_size=0.2, random_state=42, seq_len=10):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_file_path):\n",
    "        os.makedirs(output_file_path, exist_ok=True)\n",
    "\n",
    "    # Drop specified columns\n",
    "    if drop_columns:\n",
    "        data = data.drop(columns=drop_columns)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(data, seq_len)\n",
    "    \n",
    "    # Split data into training+validation sets and test set\n",
    "    X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Further split training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    X_valid_scaled = scaler.transform(X_valid.reshape(-1, X_valid.shape[-1])).reshape(X_valid.shape)\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "    # Convert numpy arrays to tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float)\n",
    "    X_valid_tensor = torch.tensor(X_valid_scaled, dtype=torch.float)\n",
    "    y_valid_tensor = torch.tensor(y_valid, dtype=torch.float)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float)\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, X_valid_tensor, y_valid_tensor, X_test_tensor, y_test_tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Long Short-Term Memory (LSTM) model class.\n",
    "    \n",
    "    This class defines an LSTM architecture with multiple LSTM layers \n",
    "    and activation functions between each layer. Dropout is also applied\n",
    "    between LSTM layers for regularization. L1 and L2 regularization \n",
    "    are supported on the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, dropout_rate=0.1, activation_function=torch.nn.ReLU(), \n",
    "                 l1_regularization=0, l2_regularization=0):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.l1_regularization = l1_regularization\n",
    "        self.l2_regularization = l2_regularization\n",
    "\n",
    "        # Build the LSTM architecture\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        self.lstm_layers.append(nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=dropout_rate, batch_first=True))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.lstm_layers.append(nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, dropout=dropout_rate, batch_first=True))\n",
    "        \n",
    "        # Final fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights of layers using Xavier initialization.\"\"\"\n",
    "\n",
    "        for layer in self.lstm_layers:\n",
    "            for param in layer.parameters():\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        \"\"\"Calculate regularization loss from model weights.\"\"\"\n",
    "\n",
    "        l1_loss = 0\n",
    "        l2_loss = 0\n",
    "\n",
    "        for param in self.parameters():\n",
    "            l1_loss += torch.norm(param, 1)\n",
    "            l2_loss += torch.norm(param, 2) ** 2\n",
    "\n",
    "        return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for the LSTM model.\"\"\"\n",
    "\n",
    "        for lstm_layer in self.lstm_layers:\n",
    "            x, _ = lstm_layer(x)\n",
    "\n",
    "        x = self.fc(x[:, -1, :])  # Taking the last output of the last LSTM layer\n",
    "\n",
    "        return x.squeeze(1)  # Squeeze to remove the singleton dimension\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import os\n",
    "\n",
    "def train_LSTM_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam, criterion=torch.nn.MSELoss, batch_size=32, learning_rate=1e-3, \n",
    "                    patience=10, min_delta=0.0001, max_norm=1.0, num_workers=0, pin_memory=False, validation_frequency=1, load_directory=None, save_directory=None, trial=1):\n",
    "\n",
    "    \"\"\"\n",
    "    Trains a Long Short-Term Memory (LSTM) model using the specified parameters and data, implementing early stopping and model saving.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The LSTM model to train.\n",
    "    - X_train, y_train: Training data and labels.\n",
    "    - X_valid, y_valid: Validation data and labels.\n",
    "    - n_epochs: Number of epochs to train the model.\n",
    "    - optimizer: The optimization algorithm (default: torch.optim.Adam).\n",
    "    - criterion: Loss function (default: torch.nn.MSELoss).\n",
    "    - batch_size: Size of batches for training and validation (default: 32).\n",
    "    - learning_rate: Learning rate for the optimizer (default: 1e-3).\n",
    "    - patience: Number of epochs with no improvement on validation loss to wait before stopping training early (default: 10).\n",
    "    - min_delta: Minimum change in validation loss to qualify as an improvement (default: 0.0001).\n",
    "    - max_norm: Maximum norm for gradient clipping (default: 1.0).\n",
    "    - num_workers: Number of subprocesses to use for data loading (default: 0).\n",
    "    - pin_memory: If True, the data loader will copy tensors into CUDA pinned memory before returning them (default: False).\n",
    "    - validation_frequency: Frequency of validation in terms of number of epochs (default: 1).\n",
    "    - load_directory: Directory from which to load the model if resuming training (default: None).\n",
    "    - save_directory: Directory where to save the model and plots (default: None).\n",
    "    - trial: Identifier for the training trial (default: 1).\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing:\n",
    "    - 'train_losses': List of average training losses per epoch.\n",
    "    - 'val_losses': List of average validation losses per validation cycle.\n",
    "    - 'best_val_loss': Best validation loss achieved during training.\n",
    "    - 'early_stopping_counter': Counter indicating the number of consecutive epochs without improvement in validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train, y_train = torch.tensor(X_train, device=device).float(), torch.tensor(y_train, device=device).float()\n",
    "    X_valid, y_valid = torch.tensor(X_valid, device=device).float(), torch.tensor(y_valid, device=device).float()\n",
    "\n",
    "    # Create DataLoader for both training and validation sets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    criterion = criterion().to(device)\n",
    "    scaler = GradScaler() # Enables mixed-precision training for faster computation.\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Lists to hold average losses per epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset gradients to zero for a new optimization step.\n",
    "            with autocast(): # Enable mixed-precision training.\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch) # Calculate loss.\n",
    "                reg_loss = model.regularization_loss() # Calculate regularization loss.\n",
    "                loss += reg_loss # Combine main loss and regularization loss.\n",
    "            scaler.scale(loss).backward() # Scale loss to prevent underflow in mixed precision.\n",
    "            scaler.unscale_(optimizer) # Unscale gradients before clipping.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm) # Clip gradients to prevent explosion.\n",
    "            scaler.step(optimizer) # Perform optimization step.\n",
    "            scaler.update() # Update scaler for next iteration.\n",
    "\n",
    "            total_train_loss += loss.item() # Accumulate the loss.\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader) # Calculate average training loss for the epoch.\n",
    "        train_losses.append(avg_train_loss)  # Store it for later analysis.\n",
    "\n",
    "    # Perform validation at the specified frequency.\n",
    "        if epoch % validation_frequency == 0:\n",
    "            model.eval() # Set the model to evaluation mode.\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():  # Disable gradient computation.\n",
    "                for X_batch, y_batch in valid_loader:\n",
    "                    y_pred = model(X_batch)\n",
    "                    loss = criterion(y_pred, y_batch) # Calculate validation loss.\n",
    "                    total_val_loss += loss.item() # Accumulate validation loss.\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(valid_loader) # Calculate average validation loss for this cycle.\n",
    "            val_losses.append(avg_val_loss)  # Store it for later analysis.\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.5f}, Val Loss = {avg_val_loss:.5f}\")\n",
    "\n",
    "            # Early stopping logic.\n",
    "            if avg_val_loss < best_val_loss - min_delta:\n",
    "                best_val_loss = avg_val_loss\n",
    "                early_stopping_counter = 0\n",
    "                if save_directory:\n",
    "                    os.makedirs(load_directory, exist_ok=True)\n",
    "                    save_path = os.path.join(load_directory, f\"LSTM_model_trial_{trial}.pt\")\n",
    "                    torch.save(model, save_path) # Save the model if there's an improvement.\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} due to no improvement in validation loss.\")\n",
    "                    break # Exit the training loop if no improvement for 'patience' epochs.\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"early_stopping_counter\": early_stopping_counter\n",
    "    }\n",
    "\n",
    "import os\n",
    "import timeit\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def model_save_LSTM(X_train, y_train, X_valid, y_valid, n_trials=1, load_directory=None, save_directory=None, plot_loss=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs multiple trials of training a Long Short-Term Memory (LSTM) model with randomly selected hyperparameters,\n",
    "    saves the trained models, and optionally plots the training and validation loss curves.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train, y_train: Training data and labels.\n",
    "    - X_valid, y_valid: Validation data and labels.\n",
    "    - n_trials (int): Number of training trials to execute with random hyperparameters.\n",
    "    - load_directory (str, optional): Directory to load existing models from (not used in current implementation).\n",
    "    - save_directory (str, optional): Directory to save trained models and results.\n",
    "    - plot_loss (bool): If True, plots the training and validation loss curves after each trial.\n",
    "\n",
    "    Returns:\n",
    "    - all_results_params_df (pandas.DataFrame): DataFrame containing the parameters and results from all trials\n",
    "      if save_directory is specified. Otherwise, returns a list of dictionaries with the same information.\n",
    "    \"\"\"\n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    all_results_params = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        # Randomly generate hyperparameters\n",
    "        hidden_size = random.choice(range(16, 128))\n",
    "        num_layers = random.choice(range(2, 6))\n",
    "        dropout_rate = random.choice([0.1, 0.2, 0.3])\n",
    "        batch_size = random.choice(range(16, 128))\n",
    "        learning_rate = 0.0001 * random.choice(range(1, 16))\n",
    "        activation_function = random.choice([torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        optimizer = random.choice([torch.optim.Adam, torch.optim.AdamW])\n",
    "        criterion = random.choice([torch.nn.SmoothL1Loss])\n",
    "        n_epochs = random.choice(range(100, 201))\n",
    "        patience = random.choice(range(5, 11))\n",
    "        min_delta = 0.0001 * random.choice(range(1, 2))\n",
    "        l1_regularization = 0.00000001 * random.choice(range(1, 2))\n",
    "        l2_regularization = l1_regularization\n",
    "\n",
    "        # Define the LSTM model\n",
    "        model = LSTMModel(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers,\n",
    "                          dropout_rate=dropout_rate, activation_function=activation_function,\n",
    "                          l1_regularization=l1_regularization, l2_regularization=l2_regularization)\n",
    "\n",
    "        # Train the LSTM model\n",
    "        training_results = train_LSTM_model(model, X_train, y_train, X_valid, y_valid, n_epochs, \n",
    "                                            batch_size=batch_size, learning_rate=learning_rate,\n",
    "                                            patience=patience, min_delta=min_delta, optimizer=optimizer, criterion=criterion, load_directory=load_directory,\n",
    "                                            save_directory=save_directory, trial=trial)\n",
    "\n",
    "        train_losses, val_losses = training_results['train_losses'], training_results['val_losses']\n",
    "        \n",
    "        # Optionally plot the loss curves\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "\n",
    "        # Collect results and parameters\n",
    "        params = {\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"activation_function\": activation_function.__class__.__name__,\n",
    "            \"optimizer\": optimizer.__name__,\n",
    "            \"criterion\": criterion.__name__,\n",
    "            \"n_epochs\": n_epochs,\n",
    "            \"patience\": patience,\n",
    "            \"min_delta\": min_delta,\n",
    "            \"l1_regularization\": l1_regularization,\n",
    "            \"l2_regularization\": l2_regularization\n",
    "        }\n",
    "\n",
    "        results_params = {**params, \"trial\": trial + 1, \"train_loss\": train_losses[-1], \"val_loss\": val_losses[-1]}\n",
    "        all_results_params.append(results_params)\n",
    "\n",
    "        # Save the results to a CSV file if a save directory is specified\n",
    "        if save_directory:\n",
    "            all_results_params_df = pd.DataFrame(all_results_params)\n",
    "            all_results_params_df.to_csv(os.path.join(save_directory, \"all_results_params.csv\"), index=False)\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        print(f\"Execution Time for Trial {trial + 1}: {end - start} seconds\")\n",
    "\n",
    "    return all_results_params_df if save_directory else all_results_params\n",
    "\n",
    "import os\n",
    "import timeit\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def evaluate_LSTM_model_and_save_results(model, X_test, y_test, load_directory=None, save_directory=None):\n",
    "    \"\"\"\n",
    "    Evaluates a trained Long Short-Term Memory (LSTM) model on test data and saves the evaluation results.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained LSTM model to evaluate.\n",
    "    - X_test, y_test: Test data and labels.\n",
    "    - load_directory (str, optional): Directory to load the trained model from.\n",
    "    - save_directory (str, optional): Directory to save the evaluation results.\n",
    "\n",
    "    Returns:\n",
    "    - eval_results (dict): Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the trained model if load_directory is specified\n",
    "    if load_directory:\n",
    "        model_path = os.path.join(load_directory, \"best_model.pt\")\n",
    "        model = torch.load(model_path)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Convert test data to PyTorch tensors\n",
    "    X_test, y_test = torch.tensor(X_test, device=device).float(), torch.tensor(y_test, device=device).float()\n",
    "\n",
    "    # Create DataLoader for test set\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "\n",
    "    # Prepare evaluation results\n",
    "    eval_results = {\n",
    "        \"test_loss\": avg_loss\n",
    "    }\n",
    "\n",
    "    # Save evaluation results if save_directory is specified\n",
    "    if save_directory:\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        eval_results_path = os.path.join(save_directory, \"evaluation_results.csv\")\n",
    "        eval_results_df = pd.DataFrame(eval_results, index=[0])\n",
    "        eval_results_df.to_csv(eval_results_path, index=False)\n",
    "\n",
    "    return eval_results\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def execute_and_evaluate_LSTM_models(data, dep_var, drop_columns, n_trials, top_k_models, save_directory, load_directory):\n",
    "    \"\"\"\n",
    "    Executes a series of steps to train, evaluate, and select the top LSTM models based on a dataset. \n",
    "    It involves data preparation, model training with random hyperparameters for a specified number of trials,\n",
    "    evaluation of these models on a test set, and selection of the top performing models based on Mean Absolute Error (MAE).\n",
    "\n",
    "    Parameters:\n",
    "    - data (pandas.DataFrame): The dataset containing features and the target variable.\n",
    "    - dep_var (str): The name of the target variable in the dataset.\n",
    "    - drop_columns (list of str): A list of column names to be dropped from the dataset before training.\n",
    "    - n_trials (int): The number of training trials to conduct with randomly selected hyperparameters.\n",
    "    - top_k_models (int): The number of top models to select based on their performance metric.\n",
    "    - save_directory (str): The directory where to save training results, model files, and evaluation results.\n",
    "    - load_directory (str): The directory from which to load trained model files for evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - top_models_df (pandas.DataFrame): A DataFrame containing the evaluation results of the top k models selected based on MAE.\n",
    "\n",
    "    This function orchestrates the workflow from data preprocessing, model training, model evaluation, \n",
    "    to the selection of the top performing models. It automates the process of experimenting with multiple \n",
    "    LSTM architectures and hyperparameters to identify models that achieve the best performance on the given dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory, exist_ok=True)  # Ensure the save directory exists.\n",
    "        \n",
    "    # Prepare data and split into training, validation, and test sets, then save the processed data.\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_and_save_data(data, dep_var, drop_columns, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train models with randomly generated hyperparameters and save the training results and plots.\n",
    "    model_save_LSTM(X_train, y_train, X_valid, y_valid, n_trials=n_trials, load_directory=load_directory, save_directory=save_directory, plot_loss=True)\n",
    "    \n",
    "    # Evaluate the trained models on the test set, calculate performance metrics, and save the evaluation results.\n",
    "    evaluate_results_df = evaluate_LSTM_model_and_save_results(X_test, y_test, load_directory=load_directory, save_directory=save_directory, results_file_name='evaluation_results.csv')\n",
    "    \n",
    "    # Select the top k models based on the lowest MAE score and print the results.\n",
    "    if 'MAE' in evaluate_results_df.columns:\n",
    "        top_models_df = evaluate_results_df.nsmallest(top_k_models, 'MAE')\n",
    "    else:\n",
    "        print(\"MAE column not found in evaluate_results_df.\")\n",
    "    \n",
    "    print(\"Top models based on MAE score:\")\n",
    "    print(top_models_df)\n",
    "    \n",
    "    # Save the evaluation results of the top models for further analysis.\n",
    "    top_models_df.to_csv(os.path.join(save_directory, \"top_models_evaluation.csv\"), index=False)\n",
    "    \n",
    "    return top_models_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the TSF file and the output CSV file\n",
    "input_file_path = r'C:\\Users\\young78703\\Documents\\GitHub\\Machine-Learning-Projects\\Data\\rideshare_dataset_without_missing_values.tsf'\n",
    "output_csv_path = r'C:\\Users\\young78703\\Documents\\GitHub\\Machine-Learning-Projects\\Data\\Time_Series\\output.csv'\n",
    "\n",
    "# Open the TSF file\n",
    "with open(input_file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Find the index where actual data starts\n",
    "data_start_index = 0\n",
    "for index, line in enumerate(lines):\n",
    "    if not line.startswith('#') and not line.startswith('@'):\n",
    "        data_start_index = index\n",
    "        break\n",
    "\n",
    "# Extract data lines and remove any leading/trailing whitespace\n",
    "data_lines = [line.strip() for line in lines[data_start_index:] if line.strip()]\n",
    "\n",
    "# Assuming the data is tab-separated, you can adjust this if needed\n",
    "df = pd.DataFrame([line.split('\\t') for line in data_lines])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"Data has been saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T0:Back Bay:Lyft:Lux:price_min:2018-11-26 06-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1:Back Bay:Lyft:Lux:price_mean:2018-11-26 06-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T2:Back Bay:Lyft:Lux:price_max:2018-11-26 06-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T3:Back Bay:Lyft:Lux:distance_min:2018-11-26 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T4:Back Bay:Lyft:Lux:distance_mean:2018-11-26 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  T0:Back Bay:Lyft:Lux:price_min:2018-11-26 06-0...\n",
       "1  T1:Back Bay:Lyft:Lux:price_mean:2018-11-26 06-...\n",
       "2  T2:Back Bay:Lyft:Lux:price_max:2018-11-26 06-0...\n",
       "3  T3:Back Bay:Lyft:Lux:distance_min:2018-11-26 0...\n",
       "4  T4:Back Bay:Lyft:Lux:distance_mean:2018-11-26 ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsf_to_csv(input_path, output_path):\n",
    "    start_reading = False\n",
    "    data = []\n",
    "    with open(input_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if start_reading:\n",
    "                if line.strip():  # Collect non-empty lines only\n",
    "                    data.append(line.strip().split(','))  # Assume comma-separated values\n",
    "            elif '@data' in line:\n",
    "                start_reading = True  # Start reading after '@data'\n",
    "\n",
    "    # Assume the file contains headers in the format described before '@data'\n",
    "    headers = ['series_name', 'source_location', 'provider_name', 'provider_service', 'type', \n",
    "               'start_timestamp', 'price_min', 'price_mean', 'price_max', 'distance_min', \n",
    "               'distance_mean', 'distance_max', 'surge_min', 'surge_mean', 'surge_max', \n",
    "               'api_calls', 'temp', 'rain', 'humidity', 'clouds', 'wind']\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(\"Data has been successfully saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "21 columns passed, passed data had 541 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 21 columns passed, passed data had 541 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m input_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124myoung78703\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMachine-Learning-Projects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrideshare_dataset_without_missing_values.tsf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124myoung78703\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMachine-Learning-Projects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTime_Series\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mread_tsf_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m, in \u001b[0;36mread_tsf_to_csv\u001b[1;34m(input_path, output_path)\u001b[0m\n\u001b[0;32m     13\u001b[0m headers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseries_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_location\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprovider_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprovider_service\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     14\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice_min\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice_mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice_max\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_min\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     15\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_max\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurge_min\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurge_mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurge_max\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     16\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_calls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumidity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclouds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwind\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[0;32m     22\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(output_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:840\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    839\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 840\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    848\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    849\u001b[0m         arrays,\n\u001b[0;32m    850\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    853\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    854\u001b[0m     )\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 21 columns passed, passed data had 541 columns"
     ]
    }
   ],
   "source": [
    "input_file_path = r'C:\\Users\\young78703\\Documents\\GitHub\\Machine-Learning-Projects\\Data\\rideshare_dataset_without_missing_values.tsf'\n",
    "output_csv_path = r'C:\\Users\\young78703\\Documents\\GitHub\\Machine-Learning-Projects\\Data\\Time_Series\\output.csv'\n",
    "read_tsf_to_csv(input_file_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_tsf_to_csv(input_path, output_path):\n",
    "    with open(input_path, 'r') as file:\n",
    "        # Skip until @data\n",
    "        for line in file:\n",
    "            if '@data' in line:\n",
    "                break\n",
    "        # Read actual data\n",
    "        data = [line.strip().split(',') for line in file if line.strip()]  # Adjust split based on actual delimiter\n",
    "\n",
    "    # Check the number of fields in the first row\n",
    "    print(\"Number of fields in the first data row:\", len(data[0]))\n",
    "\n",
    "    # Create a DataFrame with the appropriate number of columns\n",
    "    # Example with a dynamic number of columns\n",
    "    columns = [f'column_{i+1}' for i in range(len(data[0]))]  # Create dynamic column names based on data\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(\"Data has been successfully saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fields in the first data row: 541\n",
      "Data has been successfully saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "input_file_path = r'C:\\Users\\young78703\\Documents\\GitHub\\Machine-Learning-Projects\\Data\\rideshare_dataset_without_missing_values.tsf'\n",
    "output_csv_path = r'C:\\Users\\young78703\\Documents\\GitHub\\Machine-Learning-Projects\\Data\\Time_Series\\output.csv'\n",
    "read_tsf_to_csv(input_file_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T0:Back Bay:Lyft:Lux:price_min:2018-11-26 06-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1:Back Bay:Lyft:Lux:price_mean:2018-11-26 06-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T2:Back Bay:Lyft:Lux:price_max:2018-11-26 06-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T3:Back Bay:Lyft:Lux:distance_min:2018-11-26 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T4:Back Bay:Lyft:Lux:distance_mean:2018-11-26 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  T0:Back Bay:Lyft:Lux:price_min:2018-11-26 06-0...\n",
       "1  T1:Back Bay:Lyft:Lux:price_mean:2018-11-26 06-...\n",
       "2  T2:Back Bay:Lyft:Lux:price_max:2018-11-26 06-0...\n",
       "3  T3:Back Bay:Lyft:Lux:distance_min:2018-11-26 0...\n",
       "4  T4:Back Bay:Lyft:Lux:distance_mean:2018-11-26 ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_tsf_file(file_path, encoding='utf-8'):\n",
    "    metadata = {}\n",
    "    data_started = False\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not data_started:\n",
    "                if line == '@data':\n",
    "                    data_started = True\n",
    "                elif line.startswith('@'):\n",
    "                    key, value = line[1:].split(' ', 1)\n",
    "                    metadata[key] = value\n",
    "            else:\n",
    "                if line:\n",
    "                    data.append(line.split(','))  # Adjust the split as needed based on the actual delimiter\n",
    "\n",
    "    # Create a DataFrame without predefined columns to avoid errors\n",
    "    # Check if all rows have the same number of columns\n",
    "    max_cols = max(len(row) for row in data)\n",
    "    min_cols = min(len(row) for row in data)\n",
    "\n",
    "    if max_cols != min_cols:\n",
    "        print(f\"Warning: Data rows vary in length from {min_cols} to {max_cols} columns.\")\n",
    "    \n",
    "    columns = [f'column_{i+1}' for i in range(max_cols)]  # Generate column names dynamically\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    return df, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Data rows vary in length from 748 to 1008 columns.\n",
      "{'relation': 'M4', 'attribute': 'start_timestamp date', 'frequency': 'hourly', 'horizon': '48', 'missing': 'false', 'equallength': 'false'}\n",
      "                      column_1 column_2 column_3 column_4 column_5 column_6  \\\n",
      "0   T1:2015-07-01 12-00-00:605      586      586      559      511      443   \n",
      "1  T2:2015-07-01 12-00-00:3124     2990     2862     2809     2544     2201   \n",
      "2  T3:2015-07-01 12-00-00:1828     1806     1897     1750     1679     1620   \n",
      "3  T4:2015-07-01 12-00-00:6454     6324     6075     5949     5858     5579   \n",
      "4  T5:2015-07-01 12-00-00:4263     4297     4236     4080     3883     3672   \n",
      "\n",
      "  column_7 column_8 column_9 column_10  ... column_999 column_1000  \\\n",
      "0      422      395      382       370  ...       None        None   \n",
      "1     1996     1861     1735      1713  ...       None        None   \n",
      "2     1463     1342     1192      1108  ...       None        None   \n",
      "3     5163     4790     4478      4227  ...       None        None   \n",
      "4     3248     2841     2513      2275  ...       None        None   \n",
      "\n",
      "  column_1001 column_1002 column_1003 column_1004 column_1005 column_1006  \\\n",
      "0        None        None        None        None        None        None   \n",
      "1        None        None        None        None        None        None   \n",
      "2        None        None        None        None        None        None   \n",
      "3        None        None        None        None        None        None   \n",
      "4        None        None        None        None        None        None   \n",
      "\n",
      "  column_1007 column_1008  \n",
      "0        None        None  \n",
      "1        None        None  \n",
      "2        None        None  \n",
      "3        None        None  \n",
      "4        None        None  \n",
      "\n",
      "[5 rows x 1008 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "tsf_path = 'C:\\\\Users\\\\young78703\\\\Documents\\\\GitHub\\\\Machine-Learning-Projects\\\\Time_Series_Data\\\\m4_hourly_dataset.tsf'\n",
    "try:\n",
    "    dataframe, tsf_metadata = read_tsf_file(tsf_path)\n",
    "except UnicodeDecodeError:\n",
    "    # If UTF-8 decoding fails, try another encoding\n",
    "    dataframe, tsf_metadata = read_tsf_file(tsf_path, encoding='ISO-8859-1')\n",
    "\n",
    "print(tsf_metadata)\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
