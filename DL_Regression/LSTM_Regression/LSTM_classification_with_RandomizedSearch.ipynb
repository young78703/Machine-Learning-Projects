{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from joblib import dump, load\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "\n",
    "\n",
    "def read_csv_files(train_file, test_file, sep=None, header='infer'):\n",
    "    df_train = pd.read_csv(train_file, sep=sep, header=header)\n",
    "    df_test = pd.read_csv(test_file, sep=sep, header=header)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    return df_train, df_test\n",
    "\n",
    "def create_sequences(X, y, seq_len):\n",
    "    X_seq, y_seq = [], []\n",
    "\n",
    "    for i in range(X.shape[0] - seq_len + 1):\n",
    "        X_seq.append(X[i:i + seq_len, :])  # include all feature columns\n",
    "        y_seq.append(y[i + seq_len - 1])   # use -1 index instead of +1\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def prepare_data(df_train, df_test, seq_len, target_col=0, scaler=StandardScaler(), use_label_encoder=True, output_type=None):\n",
    "    if use_label_encoder:\n",
    "        label_encoder = LabelEncoder()\n",
    "        df_train[\"target\"] = label_encoder.fit_transform(df_train.iloc[:, target_col])\n",
    "        df_test[\"target\"] = label_encoder.transform(df_test.iloc[:, target_col])\n",
    "    else:\n",
    "        df_train[\"target\"] = df_train.iloc[:, target_col]\n",
    "        df_test[\"target\"] = df_test.iloc[:, target_col]\n",
    "\n",
    "    y_train = df_train[\"target\"].values\n",
    "    y_test = df_test[\"target\"].values\n",
    "\n",
    "    df_train = df_train.drop(columns=[\"target\"], axis=1)\n",
    "    df_test = df_test.drop(columns=[\"target\"], axis=1)\n",
    "\n",
    "    for col in df_train.columns:\n",
    "        df_train[col] = scaler.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "        df_test[col] = scaler.transform(df_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    X_train = df_train.values\n",
    "    X_test = df_test.values\n",
    "\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, seq_len)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, seq_len)\n",
    "\n",
    "    if output_type == \"torch\":\n",
    "        X_train, y_train = torch.tensor(X_train_seq, dtype=torch.float32), torch.tensor(y_train_seq, dtype=torch.long)\n",
    "        X_test, y_test = torch.tensor(X_test_seq, dtype=torch.float32), torch.tensor(y_test_seq, dtype=torch.long)\n",
    "    else:\n",
    "        X_train, y_train = np.array(X_train_seq), np.array(y_train_seq)\n",
    "        X_test, y_test = np.array(X_test_seq), np.array(y_test_seq)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_LSTM_classification(input_shape, unique_classes, activation_function='relu', optimizer='adam', num_hidden_layers=2,\n",
    "                               num_neurons_per_layer=64, dropout_rate=0.2, early_stop_patience=5,\n",
    "                               kernel_regularizer=None, verbose=None, target_col=None):\n",
    "\n",
    "    # Determine loss and activation for the output layer based on unique_classes\n",
    "    if unique_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "        output_activation = 'sigmoid'\n",
    "        n_output_neurons = 1\n",
    "    else:\n",
    "        loss = 'categorical_crossentropy'\n",
    "        output_activation = 'softmax'\n",
    "        n_output_neurons = unique_classes\n",
    "\n",
    "    # Initialize RNN\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(LSTM(num_neurons_per_layer, activation=activation_function,\n",
    "                       kernel_regularizer=kernel_regularizer, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(LSTM(num_neurons_per_layer, kernel_regularizer=kernel_regularizer,\n",
    "                   return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "    # Output layer (adjusted for both binary and multi-class classification)\n",
    "    model.add(Dense(n_output_neurons, activation=output_activation))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def LSTM_classification_random_search(X_train, y_train, X_test, y_test, target_col, seq_len, n_iter=1, n_times=1, cv=3,\n",
    "                                  verbose=1,dropout_rates=[0.1, 0.2, 0.3, 0.4],\n",
    "                                  activation_functions=[\"relu\", \"tanh\"],\n",
    "                                  optimizers=[\"adam\", \"rmsprop\", \"adagrad\"],\n",
    "                                  batch_sizes=[32, 64, 128], epochs=[50, 100, 200],\n",
    "                                  early_stop_patience=[5, 10, 15],\n",
    "                                  num_hidden_layers_range=range(1, 4),\n",
    "                                  neurons_per_layer_range=[16, 32, 48, 64],\n",
    "                                  learning_rate_range=[0.001, 0.01, 0.1],\n",
    "                                  model_save=True, save_directory=None,\n",
    "                                  plot_loss=True, test_size=0.3, random_state=42):\n",
    "\n",
    "  input_shape = (seq_len, X_train.shape[2])\n",
    "  unique_classes = len(np.unique(y_train))\n",
    "\n",
    "  # Model\n",
    "  Keras_classifier = KerasClassifier(model=create_LSTM_classification, input_shape=input_shape,\n",
    "                                    unique_classes=unique_classes, verbose=verbose)\n",
    "\n",
    "  # Define the random search parameters\n",
    "  param_dist = {\n",
    "      \"model__activation_function\": activation_functions,\n",
    "      \"model__dropout_rate\": dropout_rates,\n",
    "      \"model__optimizer\": optimizers,\n",
    "      \"model__num_hidden_layers\": num_hidden_layers_range,\n",
    "      \"model__num_neurons_per_layer\": neurons_per_layer_range,\n",
    "      \"model__kernel_regularizer\": [L1L2(l1=0, l2=regularization_strength) for regularization_strength in learning_rate_range],\n",
    "      \"batch_size\": batch_sizes,\n",
    "      \"epochs\": epochs,\n",
    "      \"model__early_stop_patience\": early_stop_patience\n",
    "  }\n",
    "\n",
    "  if unique_classes > 2:\n",
    "    y_train = to_categorical(y_train, num_classes=unique_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=unique_classes)\n",
    "\n",
    "  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "  all_results = []\n",
    "\n",
    "  for i in range(n_times):\n",
    "    print(f\"Iteration {i}\")\n",
    "    \n",
    "    # Create the RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(Keras_classifier, param_distributions=param_dist, n_iter=n_iter,\n",
    "                                        cv=cv, error_score=np.nan, verbose=2)\n",
    "\n",
    "    # Create EarlyStopping and ModelCheckpoint callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=early_stop_patience)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=verbose, save_best_only=False)\n",
    "\n",
    "    # Fit to the training data\n",
    "    random_search.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[es, mc])\n",
    "\n",
    "    # Get the best model from the RandomizedSearchCV\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Get the best parameters from the RandomizedSearchCV\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    # create and fit final model with best parameters\n",
    "    final_model = create_LSTM_classification(input_shape=input_shape, unique_classes=unique_classes, dropout_rate=best_params['model__dropout_rate'],\n",
    "                                            activation_function=best_params['model__activation_function'],\n",
    "                                            optimizer=best_params['model__optimizer'],\n",
    "                                            num_hidden_layers=best_params['model__num_hidden_layers'],\n",
    "                                            num_neurons_per_layer=best_params['model__num_neurons_per_layer'],\n",
    "                                            kernel_regularizer=best_params['model__kernel_regularizer'],\n",
    "                                            early_stop_patience=best_params['model__early_stop_patience'])\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=best_params['model__early_stop_patience'])\n",
    "    mc = ModelCheckpoint(f'final_model{i}.h5', monitor='val_loss', mode='min', verbose=verbose, save_best_only=True)\n",
    "    history = final_model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                              epochs=best_params['epochs'], batch_size=best_params['batch_size'], callbacks=[es, mc])\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'Loss Plot of best model_{i}')\n",
    "        plt.show()\n",
    "\n",
    "    # Save the best model\n",
    "    if save_directory is not None:\n",
    "      dump(final_model, save_directory + f'best_model{i}.joblib')\n",
    "    else:\n",
    "      dump(final_model, f'best_model{i}.joblib')\n",
    "\n",
    "    # Make predictions using the best model\n",
    "    y_pred_train = final_model.predict(X_train)\n",
    "    y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "    # Predict target values for training and test data\n",
    "    if unique_classes > 2:\n",
    "        y_pred_train = np.argmax(final_model.predict(X_train), axis=1)\n",
    "        y_pred_test = np.argmax(final_model.predict(X_test), axis=1)\n",
    "        # Use inverse_transform only if you use OneHotEncoder during preprocessing\n",
    "        y_train_original = np.argmax(y_train, axis=1)\n",
    "        y_test_original = np.argmax(y_test, axis=1)\n",
    "    else:\n",
    "        y_pred_train = (final_model.predict(X_train) > 0.5).astype('int32')\n",
    "        y_pred_test = (final_model.predict(X_test) > 0.5).astype('int32')\n",
    "        # No need for inverse_transform, use original y_train and y_test\n",
    "        y_train_original = y_train\n",
    "        y_test_original = y_test\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train_original, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test_original, y_pred_test)\n",
    "    conf_mat = confusion_matrix(y_test_original, y_pred_test)\n",
    "    class_report = classification_report(y_test_original, y_pred_test, output_dict=True, zero_division=1)\n",
    "\n",
    "    # Create a figure and axis object for the current plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot the predicted and actual values\n",
    "    ax.plot(y_pred_test, label='Predicted')\n",
    "    ax.plot(y_test_original, label='Actual')\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'Comparison of predicted and actual values')\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Create a simple report\n",
    "    simple_report = {\n",
    "        'Accuracy': round(class_report['accuracy'], 3),\n",
    "        'Precision': round(class_report['macro avg']['precision'], 3),\n",
    "        'Recall': round(class_report['macro avg']['recall'], 3),\n",
    "        'F1-score': round(class_report['macro avg']['f1-score'], 3),\n",
    "        'Supoort': round(class_report['macro avg']['support'], 3)\n",
    "    }\n",
    "\n",
    "    # Create a dataframe with results\n",
    "    results = {\n",
    "        \"Model\": [\"LSTM Classification with Randomized Search(best model_{i})\"],\n",
    "        \"Train Accuracy\": [train_accuracy],\n",
    "        \"Test Accuracy\": [test_accuracy],\n",
    "        \"Confusion Matrix\": [conf_mat.tolist()],\n",
    "        \"Classification Report\": [simple_report],\n",
    "        \"Hyperparameters\": [best_params],\n",
    "    }\n",
    "    # Append the results to the all_results list\n",
    "    all_results.append(pd.DataFrame(results))\n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    results_df = pd.concat(all_results, axis=0)\n",
    "\n",
    "  return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modification_change LSTM model and activation funciton = tanh for running cuDNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from joblib import dump, load\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "\n",
    "\n",
    "def read_csv_files(train_file, test_file, sep=None, header='infer'):\n",
    "    df_train = pd.read_csv(train_file, sep=sep, header=header)\n",
    "    df_test = pd.read_csv(test_file, sep=sep, header=header)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    return df_train, df_test\n",
    "\n",
    "def create_sequences(X, y, seq_len):\n",
    "    X_seq, y_seq = [], []\n",
    "\n",
    "    for i in range(X.shape[0] - seq_len + 1):\n",
    "        X_seq.append(X[i:i + seq_len, :])  # include all feature columns\n",
    "        y_seq.append(y[i + seq_len - 1])   # use -1 index instead of +1\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def prepare_data(df_train, df_test, seq_len, target_col=0, scaler=StandardScaler(), use_label_encoder=True, output_type=None):\n",
    "    if use_label_encoder:\n",
    "        label_encoder = LabelEncoder()\n",
    "        df_train[\"target\"] = label_encoder.fit_transform(df_train.iloc[:, target_col])\n",
    "        df_test[\"target\"] = label_encoder.transform(df_test.iloc[:, target_col])\n",
    "    else:\n",
    "        df_train[\"target\"] = df_train.iloc[:, target_col]\n",
    "        df_test[\"target\"] = df_test.iloc[:, target_col]\n",
    "\n",
    "    y_train = df_train[\"target\"].values\n",
    "    y_test = df_test[\"target\"].values\n",
    "\n",
    "    df_train = df_train.drop(columns=[\"target\"], axis=1)\n",
    "    df_test = df_test.drop(columns=[\"target\"], axis=1)\n",
    "\n",
    "    for col in df_train.columns:\n",
    "        df_train[col] = scaler.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "        df_test[col] = scaler.transform(df_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    X_train = df_train.values\n",
    "    X_test = df_test.values\n",
    "\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, seq_len)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, seq_len)\n",
    "\n",
    "    if output_type == \"torch\":\n",
    "        X_train, y_train = torch.tensor(X_train_seq, dtype=torch.float32), torch.tensor(y_train_seq, dtype=torch.long)\n",
    "        X_test, y_test = torch.tensor(X_test_seq, dtype=torch.float32), torch.tensor(y_test_seq, dtype=torch.long)\n",
    "    else:\n",
    "        X_train, y_train = np.array(X_train_seq), np.array(y_train_seq)\n",
    "        X_test, y_test = np.array(X_test_seq), np.array(y_test_seq)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_LSTM_classification(input_shape, unique_classes, activation_function='tanh', optimizer='adam', num_hidden_layers=2,\n",
    "                               num_neurons_per_layer=64, dropout_rate=0.2, early_stop_patience=5,\n",
    "                               kernel_regularizer=None, verbose=None, target_col=None):\n",
    "\n",
    "    # Determine loss and activation for the output layer based on unique_classes\n",
    "    if unique_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "        output_activation = 'sigmoid'\n",
    "        n_output_neurons = 1\n",
    "    else:\n",
    "        loss = 'categorical_crossentropy'\n",
    "        output_activation = 'softmax'\n",
    "        n_output_neurons = unique_classes\n",
    "\n",
    "    # Initialize RNN\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for i in range(num_hidden_layers):\n",
    "        if i == 0:\n",
    "            # Add the first LSTM layer with return_sequences=True for the next LSTM layer\n",
    "            model.add(LSTM(num_neurons_per_layer, activation=activation_function, recurrent_activation='sigmoid', recurrent_dropout=0, unroll=False, kernel_regularizer=kernel_regularizer, return_sequences=True))\n",
    "        else:\n",
    "            # Add intermediate LSTM layers with return_sequences=True for the next LSTM layer\n",
    "            model.add(LSTM(num_neurons_per_layer, activation=activation_function, recurrent_activation='sigmoid', recurrent_dropout=0, unroll=False, kernel_regularizer=kernel_regularizer, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(LSTM(num_neurons_per_layer, activation=activation_function, recurrent_activation='sigmoid', recurrent_dropout=0, unroll=False, kernel_regularizer=kernel_regularizer, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer (adjusted for both binary and multi-class classification)\n",
    "    model.add(Dense(n_output_neurons, activation=output_activation))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def LSTM_classification_random_search(X_train, y_train, X_test, y_test, target_col, seq_len, n_iter=1, n_times=1, cv=3,\n",
    "                                  verbose=1,dropout_rates=[0.1, 0.2, 0.3, 0.4],\n",
    "                                  activation_functions=[\"tanh\"],\n",
    "                                  optimizers=[\"adam\", \"rmsprop\", \"adagrad\"],\n",
    "                                  batch_sizes=[32, 64, 128], epochs=[50, 100, 200],\n",
    "                                  early_stop_patience=[5, 10, 15],\n",
    "                                  num_hidden_layers_range=range(1, 4),\n",
    "                                  neurons_per_layer_range=[16, 32, 48, 64],\n",
    "                                  learning_rate_range=[0.001, 0.01, 0.1],\n",
    "                                  model_save=True, save_directory=None,\n",
    "                                  plot_loss=True, test_size=0.3, random_state=42):\n",
    "\n",
    "  input_shape = (seq_len, X_train.shape[2])\n",
    "  unique_classes = len(np.unique(y_train))\n",
    "\n",
    "  # Model\n",
    "  Keras_classifier = KerasClassifier(model=create_LSTM_classification, input_shape=input_shape,\n",
    "                                    unique_classes=unique_classes, verbose=verbose)\n",
    "\n",
    "  # Define the random search parameters\n",
    "  param_dist = {\n",
    "      \"model__activation_function\": activation_functions,\n",
    "      \"model__dropout_rate\": dropout_rates,\n",
    "      \"model__optimizer\": optimizers,\n",
    "      \"model__num_hidden_layers\": num_hidden_layers_range,\n",
    "      \"model__num_neurons_per_layer\": neurons_per_layer_range,\n",
    "      \"model__kernel_regularizer\": [L1L2(l1=0, l2=regularization_strength) for regularization_strength in learning_rate_range],\n",
    "      \"batch_size\": batch_sizes,\n",
    "      \"epochs\": epochs,\n",
    "      \"model__early_stop_patience\": early_stop_patience\n",
    "  }\n",
    "\n",
    "  if unique_classes > 2:\n",
    "    y_train = to_categorical(y_train, num_classes=unique_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=unique_classes)\n",
    "\n",
    "  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "  all_results = []\n",
    "\n",
    "  for i in range(n_times):\n",
    "    print(f\"Iteration {i}\")\n",
    "    \n",
    "    # Create the RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(Keras_classifier, param_distributions=param_dist, n_iter=n_iter,\n",
    "                                        cv=cv, error_score=np.nan, verbose=2)\n",
    "\n",
    "    # Create EarlyStopping and ModelCheckpoint callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=early_stop_patience)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=verbose, save_best_only=False)\n",
    "\n",
    "    # Fit to the training data\n",
    "    random_search.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[es, mc])\n",
    "\n",
    "    # Get the best model from the RandomizedSearchCV\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Get the best parameters from the RandomizedSearchCV\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    # create and fit final model with best parameters\n",
    "    final_model = create_LSTM_classification(input_shape=input_shape, unique_classes=unique_classes, dropout_rate=best_params['model__dropout_rate'],\n",
    "                                            activation_function=best_params['model__activation_function'],\n",
    "                                            optimizer=best_params['model__optimizer'],\n",
    "                                            num_hidden_layers=best_params['model__num_hidden_layers'],\n",
    "                                            num_neurons_per_layer=best_params['model__num_neurons_per_layer'],\n",
    "                                            kernel_regularizer=best_params['model__kernel_regularizer'],\n",
    "                                            early_stop_patience=best_params['model__early_stop_patience'])\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=best_params['model__early_stop_patience'])\n",
    "    mc = ModelCheckpoint(f'final_model{i}.h5', monitor='val_loss', mode='min', verbose=verbose, save_best_only=True)\n",
    "    history = final_model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                              epochs=best_params['epochs'], batch_size=best_params['batch_size'], callbacks=[es, mc])\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'Loss Plot of best model_{i}')\n",
    "        plt.show()\n",
    "\n",
    "    # Save the best model\n",
    "    if save_directory is not None:\n",
    "      dump(final_model, save_directory + f'best_model{i}.joblib')\n",
    "    else:\n",
    "      dump(final_model, f'best_model{i}.joblib')\n",
    "\n",
    "    # Make predictions using the best model\n",
    "    y_pred_train = final_model.predict(X_train)\n",
    "    y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "    # Predict target values for training and test data\n",
    "    if unique_classes > 2:\n",
    "        y_pred_train = np.argmax(final_model.predict(X_train), axis=1)\n",
    "        y_pred_test = np.argmax(final_model.predict(X_test), axis=1)\n",
    "        # Use inverse_transform only if you use OneHotEncoder during preprocessing\n",
    "        y_train_original = np.argmax(y_train, axis=1)\n",
    "        y_test_original = np.argmax(y_test, axis=1)\n",
    "    else:\n",
    "        y_pred_train = (final_model.predict(X_train) > 0.5).astype('int32')\n",
    "        y_pred_test = (final_model.predict(X_test) > 0.5).astype('int32')\n",
    "        # No need for inverse_transform, use original y_train and y_test\n",
    "        y_train_original = y_train\n",
    "        y_test_original = y_test\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train_original, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test_original, y_pred_test)\n",
    "    conf_mat = confusion_matrix(y_test_original, y_pred_test)\n",
    "    class_report = classification_report(y_test_original, y_pred_test, output_dict=True, zero_division=1)\n",
    "\n",
    "    # Create a figure and axis object for the current plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot the predicted and actual values\n",
    "    ax.plot(y_pred_test, label='Predicted')\n",
    "    ax.plot(y_test_original, label='Actual')\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'Comparison of predicted and actual values')\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Create a simple report\n",
    "    simple_report = {\n",
    "        'Accuracy': round(class_report['accuracy'], 3),\n",
    "        'Precision': round(class_report['macro avg']['precision'], 3),\n",
    "        'Recall': round(class_report['macro avg']['recall'], 3),\n",
    "        'F1-score': round(class_report['macro avg']['f1-score'], 3),\n",
    "        'Supoort': round(class_report['macro avg']['support'], 3)\n",
    "    }\n",
    "\n",
    "    # Create a dataframe with results\n",
    "    results = {\n",
    "        \"Model\": [\"LSTM Classification with Randomized Search(best model_{i})\"],\n",
    "        \"Train Accuracy\": [train_accuracy],\n",
    "        \"Test Accuracy\": [test_accuracy],\n",
    "        \"Confusion Matrix\": [conf_mat.tolist()],\n",
    "        \"Classification Report\": [simple_report],\n",
    "        \"Hyperparameters\": [best_params],\n",
    "    }\n",
    "    # Append the results to the all_results list\n",
    "    all_results.append(pd.DataFrame(results))\n",
    "    # Concatenate all the results into a single DataFrame\n",
    "    results_df = pd.concat(all_results, axis=0)\n",
    "\n",
    "  return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8926, 97) (7711, 97)\n"
     ]
    }
   ],
   "source": [
    "# Read the files\n",
    "train_file = \"/home/young78703/Data_Science_Project/data/UCRArchive_2018/ElectricDevices/ElectricDevices_TRAIN.tsv\"\n",
    "test_file = \"/home/young78703/Data_Science_Project/data/UCRArchive_2018/ElectricDevices/ElectricDevices_TEST.tsv\"\n",
    "df_train, df_test = read_csv_files(train_file, test_file, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "seq_len = 3\n",
    "use_label_encoder=True\n",
    "output_type=None\n",
    "X_train, y_train, X_test, y_test = prepare_data(df_train, df_test, seq_len=seq_len, target_col=0, output_type=output_type, use_label_encoder=use_label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_classification_random_search(X_train, y_train, X_test, y_test, seq_len=3, target_col=0, n_iter=1, n_times=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
